# FIF-A é¡¹ç›®æŠ€æœ¯æ”¹è¿›æ–¹æ¡ˆ

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**ç¼–åˆ¶æ—¥æœŸ**: 2025-12-02
**é¡¹ç›®çŠ¶æ€**: ç ”ç©¶åŸå‹ â†’ ç”Ÿäº§å°±ç»ª
**æ”¹è¿›åŸåˆ™**: é›¶ç ´åæ€§ï¼Œä¿æŒè®¡ç®—é€»è¾‘ä¸å˜ï¼Œæ¸è¿›å¼ä¼˜åŒ–

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–¹æ¡ˆé’ˆå¯¹ FIF-A é¡¹ç›®çš„ 24 é¡¹æŠ€æœ¯å€ºåŠ¡æå‡ºç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆã€‚æ‰€æœ‰æ”¹è¿›**ä¸¥æ ¼éµå¾ªé›¶ç ´åæ€§åŸåˆ™**ï¼š
- âœ… ä¿æŒç°æœ‰è¿ç®—é€»è¾‘å®Œå…¨ä¸å˜
- âœ… ä¿è¯æ•°å€¼ç»“æœé€ä½ä¸€è‡´
- âœ… å‘åå…¼å®¹ç°æœ‰API
- âœ… æ¸è¿›å¼å®æ–½ï¼Œå¯éšæ—¶å›æ»š

**é¢„æœŸæ”¶ç›Š**:
- ğŸ§ª æµ‹è¯•è¦†ç›–ç‡ 0% â†’ 80%+
- ğŸ’¾ æ¶ˆé™¤å†…å­˜æ³„æ¼é£é™©ï¼ˆç¼“å­˜æ— é™å¢é•¿ï¼‰
- ğŸš€ æ€§èƒ½æå‡ 15-30%ï¼ˆGPUä¼˜åŒ–ï¼‰
- ğŸ›¡ï¸ æ¶ˆé™¤ 3 å¤„é™é»˜å¤±è´¥é£é™©
- ğŸ“¦ ä¾èµ–å¯å¤ç°æ€§ 100%

---

## ğŸ¯ æ”¹è¿›èŒƒå›´ç•Œå®š

### åŒ…å«çš„æ”¹è¿›
- æµ‹è¯•åŸºç¡€è®¾æ–½å»ºè®¾
- æ€§èƒ½ç“¶é¢ˆä¼˜åŒ–
- å¼‚å¸¸å¤„ç†å¢å¼º
- ä¾èµ–ç‰ˆæœ¬é”å®š
- ä»£ç é‡æ„ï¼ˆéæ ¸å¿ƒé€»è¾‘ï¼‰

### æ˜ç¡®æ’é™¤çš„æ”¹è¿›
- âŒ ä¸ä¿®æ”¹æ‘©æ“¦å±‚æ•°å­¦å…¬å¼
- âŒ ä¸æ”¹å˜èƒ½é‡è®¡ç®—ç®—æ³•
- âŒ ä¸è°ƒæ•´è®­ç»ƒè¶…å‚æ•°
- âŒ ä¸ä¿®æ”¹æ¨¡å‹æ¶æ„
- âŒ ä¸é‡å†™æ ¸å¿ƒå‰å‘/åå‘ä¼ æ’­

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šé—®é¢˜æ¸…å•ä¸æŠ€æœ¯å€ºåŠ¡

## ğŸ”´ P0 çº§é—®é¢˜ï¼ˆä¸¥é‡ï¼Œå¿…é¡»ä¿®å¤ï¼‰

### é—®é¢˜ 1: å…¨å±€ç¼“å­˜æ— é™å¢é•¿å¯¼è‡´å†…å­˜æ³„æ¼ï¼ˆå·²å®Œæˆï¼šv1.1.0 å¼€å‘åˆ†æ”¯åŠ å…¥æœ‰ç•Œç¼“å­˜ + clear æ¥å£ï¼‰

**ä½ç½®**: `fif_mvp/utils/sparse.py:15-16`

**é—®é¢˜æè¿°**:
```python
_WINDOW_CACHE: Dict[Tuple[int, int], torch.Tensor] = {}
_WINDOW_CACHE_DEVICE: Dict[Tuple[int, int, str], torch.Tensor] = {}
```

**å½±å“åˆ†æ**:
- æ¯ä¸ªæ–°çš„ `(length, radius)` ç»„åˆæ°¸ä¹…å­˜å‚¨
- SST-2 å¹³å‡é•¿åº¦ 19ï¼Œæœ€é•¿ 52 â†’ ç†è®ºç¼“å­˜é¡¹ 52 ä¸ª
- SNLI å¹³å‡é•¿åº¦ 14ï¼Œæœ€é•¿ 82 â†’ ç†è®ºç¼“å­˜é¡¹ 82 ä¸ª
- å¤š GPU è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªè®¾å¤‡é‡å¤ç¼“å­˜ â†’ å®é™…å ç”¨ Ã— GPU æ•°
- é•¿æ—¶é—´è®­ç»ƒï¼ˆ100+ epochsï¼‰å¯èƒ½ç´¯ç§¯æ•°ç™¾ MB æ— ç”¨ç¼“å­˜

**é£é™©ç­‰çº§**: é«˜ï¼ˆç”Ÿäº§ç¯å¢ƒå¯èƒ½ OOMï¼‰

**ç°æœ‰ä»£ç è¡Œä¸ºéªŒè¯**:
```python
# å½“å‰è¡Œä¸ºï¼šæ— é™åˆ¶ç¼“å­˜
length=50, radius=3 â†’ ç¼“å­˜ tensor(shape=[147, 2])  # ~1.2KB
length=51, radius=3 â†’ æ–°å»ºç¼“å­˜ tensor(shape=[150, 2])  # +1.2KB
# ... æ— ä¸Šé™ç´¯ç§¯
```

---

### é—®é¢˜ 2: GPUâ†”CPU æ•°æ®ä¼ è¾“ç“¶é¢ˆï¼ˆå·²å®Œæˆï¼šv1.1.0 å¼€å‘åˆ†æ”¯ç§»é™¤ tolist CPU å›é€€ï¼‰

**ä½ç½®**: `fif_mvp/models/friction_layer.py:45`

**é—®é¢˜æè¿°**:
```python
lengths = attention_mask.sum(dim=1).to(torch.int64)
buckets: dict[int, List[int]] = defaultdict(list)
for idx, length in enumerate(lengths.tolist()):  # â† tolist() è§¦å‘ GPUâ†’CPU ä¼ è¾“
    buckets[int(length)].append(idx)
```

**æ€§èƒ½å½±å“æµ‹é‡**:
```
åœºæ™¯ï¼šbatch_size=32, avg_length=20
- lengths.tolist(): ~0.5ms (GPUâ†’CPU åŒæ­¥)
- Python for å¾ªç¯: ~0.3ms
- æ€»è®¡æ¯ batch æŸå¤±: ~0.8ms
- å‡è®¾ 10,000 batches/epoch â†’ æµªè´¹ 8 ç§’/epoch
```

**é£é™©ç­‰çº§**: ä¸­ï¼ˆè®­ç»ƒæ•ˆç‡é™ä½ 5-10%ï¼‰

**æ•°å€¼å…¼å®¹æ€§ä¿è¯**:
- çº¯ç²¹æ€§èƒ½ä¼˜åŒ–ï¼Œä¸æ”¹å˜åˆ†æ¡¶ç»“æœ
- ä¼˜åŒ–åçš„æ¡¶åˆ†é…ä¸åŸé€»è¾‘å®Œå…¨ä¸€è‡´

---

### é—®é¢˜ 3: æ··æ·†çŸ©é˜µè®¡ç®—æœªå‘é‡åŒ–ï¼ˆå·²å®Œæˆï¼šv1.1.0 å¼€å‘åˆ†æ”¯æ”¹ä¸º bincount å‘é‡åŒ–ï¼‰

**ä½ç½®**: `fif_mvp/train/metrics.py:41-43`

**é—®é¢˜æè¿°**:
```python
matrix = np.zeros((num_labels, num_labels), dtype=int)
for y_true, y_pred in zip(labels, preds):  # â† Python å¾ªç¯
    matrix[y_true, y_pred] += 1
```

**æ€§èƒ½å½±å“**:
- SST-2 æµ‹è¯•é›† 872 æ ·æœ¬ â†’ å¾ªç¯ 872 æ¬¡
- SNLI æµ‹è¯•é›† 10,000 æ ·æœ¬ â†’ å¾ªç¯ 10,000 æ¬¡
- æ¯ä¸ª epoch è¯„ä¼°æŸå¤± ~50-100ms

**é£é™©ç­‰çº§**: ä½ï¼ˆè¯„ä¼°é˜¶æ®µéå…³é”®è·¯å¾„ï¼‰

**æ•°å€¼å…¼å®¹æ€§**: NumPy å‘é‡åŒ–æ“ä½œä¿è¯æ•´æ•°ç²¾åº¦

---

### é—®é¢˜ 4: ä¾èµ–ç‰ˆæœ¬æœªå›ºå®šï¼ˆå·²å®Œæˆï¼šrequirements é”å®šæ¬¡è¦ç‰ˆæœ¬èŒƒå›´ï¼‰

**ä½ç½®**: `requirements.txt:1-7`

**é—®é¢˜æè¿°**:
```
torch>=2.2        # å¯èƒ½å®‰è£… 2.3, 2.4, 3.0...
transformers>=4.44 # API å¯èƒ½ç ´åæ€§å˜æ›´
```

**é£é™©åœºæ™¯**:
```bash
# ç ”ç©¶è€… A (2025-01)
pip install torch>=2.2  â†’ å®‰è£… torch==2.2.0
python run.py --dataset sst2  â†’ å‡†ç¡®ç‡ 94.2%

# ç ”ç©¶è€… B (2025-06)
pip install torch>=2.2  â†’ å®‰è£… torch==2.5.0  # æ–°ç‰ˆæœ¬
python run.py --dataset sst2  â†’ å‡†ç¡®ç‡ 93.8%  # ç»“æœä¸å¯å¤ç°ï¼
```

**é£é™©ç­‰çº§**: é«˜ï¼ˆç§‘ç ”å¯å¤ç°æ€§æ ¸å¿ƒï¼‰

---

### é—®é¢˜ 5: é™é»˜å¤±è´¥éšè—æ½œåœ¨é”™è¯¯ï¼ˆå·²å®Œæˆï¼šå…³é”®è·¯å¾„æ”¹ä¸º emit_warningï¼‰

**ä½ç½®**: `fif_mvp/cli/run_experiment.py:472-473`

**é—®é¢˜æè¿°**:
```python
try:
    if hasattr(torch, "set_float32_matmul_precision") and major >= 8:
        torch.set_float32_matmul_precision("high")
except Exception:  # â† æ•è·æ‰€æœ‰å¼‚å¸¸
    pass           # â† å®Œå…¨å¿½ç•¥
```

**é£é™©åœºæ™¯**:
```python
# å‡è®¾ torch ç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´ AttributeError
# æˆ–è€… CUDA é©±åŠ¨é—®é¢˜å¯¼è‡´ RuntimeError
# å½“å‰è¡Œä¸ºï¼šé™é»˜è·³è¿‡ï¼Œç”¨æˆ·æ¯«æ— æ„ŸçŸ¥
# æœŸæœ›è¡Œä¸ºï¼šè®°å½•è­¦å‘Šæ—¥å¿—ï¼Œä¾¿äºè°ƒè¯•
```

**é£é™©ç­‰çº§**: ä¸­ï¼ˆè°ƒè¯•å›°éš¾ï¼Œéšè—é…ç½®é—®é¢˜ï¼‰

---

## ğŸŸ¡ P1 çº§é—®é¢˜ï¼ˆé‡è¦ï¼Œåº”è¯¥ä¿®å¤ï¼‰

### é—®é¢˜ 6: è¶…é•¿å‡½æ•°éš¾ä»¥ç»´æŠ¤ï¼ˆå·²å®Œæˆï¼šv1.1.0 å¼€å‘åˆ†æ”¯æ‹†åˆ† `_run_cli` ä¸ºå¤šæ®µ helperï¼‰

**ä½ç½®**: `fif_mvp/cli/run_experiment.py:329-521` (_run_cli å‡½æ•° 193 è¡Œ â†’ ç°å·²æ‹†åˆ†è‡³ <80 è¡Œï¼Œæ–°å¢ `_initialize_device_choice`ã€`_build_experiment_config`ã€`_build_data_bundle` ç­‰ helper)

**é—®é¢˜æè¿°**:
- åœˆå¤æ‚åº¦ > 15
- æ··åˆäº† 10+ é¡¹èŒè´£ï¼šå‚æ•°è§£æã€è®¾å¤‡åˆå§‹åŒ–ã€æ¨¡å‹åˆ›å»ºã€æ•°æ®åŠ è½½ã€è®­ç»ƒå¾ªç¯ã€ç»“æœä¿å­˜
- å•å…ƒæµ‹è¯•å›°éš¾ï¼ˆæ— æ³•ç‹¬ç«‹æµ‹è¯•å­åŠŸèƒ½ï¼‰

**ç»´æŠ¤æˆæœ¬**:
```python
# å½“å‰ï¼šä¿®æ”¹è®¾å¤‡åˆå§‹åŒ–é€»è¾‘ â†’ éœ€è¦ç†è§£æ•´ä¸ª 193 è¡Œå‡½æ•°
# æœŸæœ›ï¼šä¿®æ”¹ _initialize_device() â†’ ä»…éœ€ç†è§£ 20 è¡Œå‡½æ•°
```

**é£é™©ç­‰çº§**: ä¸­ï¼ˆé•¿æœŸç»´æŠ¤æˆæœ¬é«˜ï¼‰

---

### é—®é¢˜ 7: ä»£ç é‡å¤å¯¼è‡´ç»´æŠ¤ä¸ä¸€è‡´ï¼ˆå·²å®Œæˆï¼šå…±ç”¨ `build_loaders_for_splits`ï¼‰

**ä½ç½®**:
- `fif_mvp/data/sst2.py:59-84`
- `fif_mvp/data/snli.py:119-146`

**é—®é¢˜æè¿°**:
```python
# sst2.py å’Œ snli.py ä¸­ç›¸åŒçš„ DataLoader åˆ›å»ºé€»è¾‘ï¼ˆ26 è¡Œé‡å¤ï¼‰
def get_loaders(...):
    # ... å®Œå…¨ç›¸åŒçš„ collate_fn å®šä¹‰
    # ... å®Œå…¨ç›¸åŒçš„ DataLoader å‚æ•°
    # ... å®Œå…¨ç›¸åŒçš„ worker æ•°é‡è®¡ç®—
```

**ç»´æŠ¤é£é™©**:
```python
# åœºæ™¯ï¼šéœ€è¦ä¿®å¤ DataLoader çš„ pin_memory bug
# å½“å‰ï¼šå¿…é¡»åŒæ­¥ä¿®æ”¹ sst2.py å’Œ snli.py ä¸¤å¤„
# é£é™©ï¼šå®¹æ˜“é—æ¼ä¸€å¤„ï¼Œå¯¼è‡´ä¸ä¸€è‡´è¡Œä¸º
```

**é£é™©ç­‰çº§**: ä¸­ï¼ˆbug ä¿®å¤å®¹æ˜“é—æ¼ï¼‰

---

### é—®é¢˜ 8: å¼‚å¸¸æ•è·è¿‡äºå®½æ³›

**ä½ç½®**: `fif_mvp/cli/run_experiment.py:320`

**é—®é¢˜æè¿°**:
```python
try:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
except Exception as exc:  # â† æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ŒåŒ…æ‹¬ SystemExit, KeyboardInterrupt
    raise RuntimeError("CUDA init failed") from exc
```

**æœ€ä½³å®è·µ**:
```python
# åº”è¯¥ä»…æ•è·é¢„æœŸçš„å¼‚å¸¸ç±»å‹
except (RuntimeError, torch.cuda.CudaError) as exc:
```

**é£é™©ç­‰çº§**: ä½ï¼ˆä½†è¿å Python æœ€ä½³å®è·µï¼‰

---

## ğŸŸ¢ P2 çº§é—®é¢˜ï¼ˆä¼˜åŒ–ï¼Œå»ºè®®ä¿®å¤ï¼‰

### é—®é¢˜ 9-24: å·¥ç¨‹è´¨é‡é—®é¢˜

| ID | é—®é¢˜ | ä½ç½® | å½±å“ |
|-----|------|------|------|
| 9 | ç¼ºå°‘ç±»å‹æ³¨è§£ | 95 ä¸ªå‡½æ•° | å¼€å‘ä½“éªŒå·® |
| 10 | Magic numbers | `data/__init__.py:119` | é…ç½®ä¸é€æ˜ |
| 11 | print/logging æ··ç”¨ | CLI å…¨å±€ | æ—¥å¿—ä¸è§„èŒƒ |
| 12 | ç¼ºå°‘æµ‹è¯• | æ•´ä¸ªé¡¹ç›® | è´¨é‡æ— ä¿éšœ |
| 13 | ç¼ºå°‘ CI/CD | `.github/` | æ— è‡ªåŠ¨åŒ– |
| 14 | å˜é‡å‘½åä¸ä¸€è‡´ | å¤šå¤„ | å¯è¯»æ€§å·® |
| 15 | ç¼ºå°‘æ–‡æ¡£æ³¨é‡Š | å¤æ‚ç®—æ³• | ç†è§£æˆæœ¬é«˜ |

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šè§£å†³æ–¹æ¡ˆè¯¦ç»†è®¾è®¡

## è§£å†³æ–¹æ¡ˆ 1: ä¿®å¤ç¼“å­˜å†…å­˜æ³„æ¼

### æ–¹æ¡ˆè®¾è®¡

**ç›®æ ‡**: é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢æ— é™å¢é•¿ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ä¼˜åŠ¿

**æŠ€æœ¯é€‰å‹**: ä½¿ç”¨ `functools.lru_cache`ï¼ˆPython æ ‡å‡†åº“ï¼‰

**å®æ–½æ–¹æ¡ˆ**:

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/utils/sparse.py
# ä¿®æ”¹èŒƒå›´: ç¬¬ 15-54 è¡Œ
# ============================================================

# ====== ä¿®æ”¹å‰ ======
_WINDOW_CACHE: Dict[Tuple[int, int], torch.Tensor] = {}
_WINDOW_CACHE_DEVICE: Dict[Tuple[int, int, str], torch.Tensor] = {}

def build_window_edges(length: int, radius: int, device: Optional[torch.device] = None):
    key = (length, radius)
    cached = _WINDOW_CACHE.get(key)
    if cached is None:
        # ... æ„å»ºé€»è¾‘
        _WINDOW_CACHE[key] = cached
    # ... è®¾å¤‡è½¬æ¢
    _WINDOW_CACHE_DEVICE[key_dev] = dev_cached
    return dev_cached

# ====== ä¿®æ”¹å ======
from functools import lru_cache

@lru_cache(maxsize=128)  # é™åˆ¶æœ€å¤š 128 ä¸ªä¸åŒ (length, radius) ç»„åˆ
def _build_window_edges_cpu(length: int, radius: int) -> torch.Tensor:
    """æ„å»ºæ»‘åŠ¨çª—å£è¾¹ï¼ˆCPU ç‰ˆæœ¬ï¼Œå¯å“ˆå¸Œå‚æ•°ï¼‰ã€‚

    æ­¤å‡½æ•°è¢« lru_cache è£…é¥°ï¼Œè‡ªåŠ¨ç®¡ç†ç¼“å­˜æ·˜æ±°ç­–ç•¥ã€‚
    """
    if length <= 1 or radius <= 0:
        return torch.zeros((0, 2), dtype=torch.long)

    edges: List[Tuple[int, int]] = []
    for i in range(length):
        lo = max(0, i - radius)
        hi = min(length, i + radius + 1)
        for j in range(lo, hi):
            if i < j:
                edges.append((i, j))

    if not edges:
        return torch.zeros((0, 2), dtype=torch.long)
    return torch.tensor(edges, dtype=torch.long)


# è®¾å¤‡ç¼“å­˜ä¿æŒæ‰‹åŠ¨ç®¡ç†ï¼ˆå› ä¸º torch.device ä¸å¯å“ˆå¸Œï¼‰
_DEVICE_CACHE: Dict[Tuple[int, int, str], torch.Tensor] = {}
_DEVICE_CACHE_MAX_SIZE = 256  # æ–°å¢ï¼šæœ€å¤§ç¼“å­˜é¡¹æ•°

def build_window_edges(
    length: int, radius: int, device: Optional[torch.device] = None
) -> torch.Tensor:
    """è¿”å›æ»‘åŠ¨çª—å£çš„æ— å‘è¾¹ï¼ŒæŒ‰ (length, radius) ç¼“å­˜ã€‚

    å˜æ›´è¯´æ˜ï¼š
    - CPU ç¼“å­˜ä½¿ç”¨ lru_cacheï¼ˆæœ€å¤š 128 é¡¹ï¼‰
    - è®¾å¤‡ç¼“å­˜æ‰‹åŠ¨ç®¡ç†ï¼ˆæœ€å¤š 256 é¡¹ï¼ŒLRU æ·˜æ±°ï¼‰
    - ä¿è¯æ•°å€¼ç»“æœä¸åŸå®ç°å®Œå…¨ä¸€è‡´
    """
    # 1. è·å– CPU ç¼“å­˜ï¼ˆé€šè¿‡ lru_cacheï¼‰
    cached_cpu = _build_window_edges_cpu(length, radius)

    # 2. å¦‚æœæ˜¯ CPU è®¾å¤‡ï¼Œç›´æ¥è¿”å›
    if device is None or str(device) == "cpu":
        return cached_cpu

    # 3. è®¾å¤‡ç¼“å­˜æŸ¥æ‰¾
    key_dev = (length, radius, str(device))
    dev_cached = _DEVICE_CACHE.get(key_dev)

    # 4. ç¼“å­˜æœªå‘½ä¸­æˆ–è®¾å¤‡ä¸åŒ¹é…ï¼Œæ‰§è¡Œè½¬æ¢
    if dev_cached is None or dev_cached.device != device:
        dev_cached = cached_cpu.to(device, non_blocking=(device.type == "cuda"))

        # ç¼“å­˜å¤§å°é™åˆ¶ï¼ˆLRU æ·˜æ±°æœ€æ—§é¡¹ï¼‰
        if len(_DEVICE_CACHE) >= _DEVICE_CACHE_MAX_SIZE:
            # ç§»é™¤æœ€æ—©æ’å…¥çš„é¡¹ï¼ˆPython 3.7+ å­—å…¸ä¿æŒæ’å…¥é¡ºåºï¼‰
            oldest_key = next(iter(_DEVICE_CACHE))
            del _DEVICE_CACHE[oldest_key]

        _DEVICE_CACHE[key_dev] = dev_cached

    return dev_cached
```

### éªŒè¯æ–¹æ¡ˆ

**éªŒè¯ 1: æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•**

```python
# tests/test_sparse_cache_fix.py
import torch
from fif_mvp.utils.sparse import build_window_edges

def test_cache_fix_numerical_equivalence():
    """éªŒè¯ç¼“å­˜ä¿®å¤åï¼Œç»“æœä¸åŸé€»è¾‘å®Œå…¨ä¸€è‡´"""
    # ä½¿ç”¨åŸä»£ç ä¿å­˜çš„å‚è€ƒè¾“å‡º
    reference_output = torch.load("tests/fixtures/window_edges_reference.pt")

    for length, radius in [(10, 2), (50, 3), (100, 5)]:
        result = build_window_edges(length, radius)
        expected = reference_output[(length, radius)]
        assert torch.equal(result, expected), f"ä¸åŒ¹é…: length={length}, radius={radius}"

def test_cache_memory_bounded():
    """éªŒè¯ç¼“å­˜ä¸ä¼šæ— é™å¢é•¿"""
    import sys
    from fif_mvp.utils import sparse

    # æ¸…ç©ºç¼“å­˜
    sparse._build_window_edges_cpu.cache_clear()
    sparse._DEVICE_CACHE.clear()

    # ç”Ÿæˆ 200 ä¸ªä¸åŒçš„ (length, radius) ç»„åˆ
    for length in range(10, 210):
        build_window_edges(length, radius=3)

    # éªŒè¯ CPU ç¼“å­˜ä¸è¶…è¿‡ 128
    cache_info = sparse._build_window_edges_cpu.cache_info()
    assert cache_info.currsize <= 128, f"CPU ç¼“å­˜è¶…é™: {cache_info.currsize}"

    # éªŒè¯è®¾å¤‡ç¼“å­˜ä¸è¶…è¿‡ 256
    assert len(sparse._DEVICE_CACHE) <= 256, f"è®¾å¤‡ç¼“å­˜è¶…é™: {len(sparse._DEVICE_CACHE)}"
```

**éªŒè¯ 2: æ€§èƒ½åŸºå‡†æµ‹è¯•**

```python
# tests/benchmark_cache_performance.py
import time
import torch
from fif_mvp.utils.sparse import build_window_edges

def benchmark_cache_hit_rate():
    """éªŒè¯ç¼“å­˜å‘½ä¸­ç‡ä¿æŒåœ¨ 95%+"""
    lengths = [19, 20, 21, 22] * 250  # æ¨¡æ‹Ÿ SST-2 çœŸå®åˆ†å¸ƒ

    start = time.perf_counter()
    for length in lengths:
        build_window_edges(length, radius=3, device=torch.device("cuda"))
    elapsed = time.perf_counter() - start

    # æœŸæœ›ï¼š1000 æ¬¡è°ƒç”¨ä¸­ï¼Œ96% å‘½ä¸­ç¼“å­˜ï¼Œè€—æ—¶ < 10ms
    assert elapsed < 0.01, f"ç¼“å­˜æ€§èƒ½é€€åŒ–: {elapsed:.3f}s"
    print(f"âœ“ ç¼“å­˜æ€§èƒ½æµ‹è¯•é€šè¿‡: {elapsed*1000:.2f}ms for 1000 calls")
```

### éƒ¨ç½²è®¡åˆ’

**é˜¶æ®µ 1: å½±å­æµ‹è¯•ï¼ˆ1 å‘¨ï¼‰**
```bash
# 1. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
git checkout -b fix/cache-memory-leak

# 2. å®æ–½ä¿®æ”¹
# ï¼ˆåº”ç”¨ä¸Šè¿°ä»£ç ï¼‰

# 3. è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
pytest tests/test_sparse_cache_fix.py -v
python tests/benchmark_cache_performance.py

# 4. ç«¯åˆ°ç«¯éªŒè¯ï¼ˆè¿è¡ŒçœŸå®è®­ç»ƒï¼‰
python -m fif_mvp.cli.run_experiment \
    --dataset sst2 \
    --epochs 5 \
    --save_dir result/cache_fix_validation

# 5. å¯¹æ¯”ç»“æœï¼ˆä¸ä¸»åˆ†æ”¯çš„ç»“æœæ–‡ä»¶ï¼‰
python scripts/compare_training_results.py \
    result/baseline/metrics.json \
    result/cache_fix_validation/metrics.json \
    --tolerance 1e-6  # å…è®¸æµ®ç‚¹è¯¯å·®
```

**é˜¶æ®µ 2: ä»£ç å®¡æŸ¥ï¼ˆ3 å¤©ï¼‰**
- æäº¤ PRï¼Œé™„ä¸Šæ€§èƒ½æµ‹è¯•æŠ¥å‘Š
- è‡³å°‘ 1 ä½æ ¸å¿ƒå¼€å‘è€…å®¡æŸ¥
- ç¡®è®¤æ‰€æœ‰ CI æ£€æŸ¥é€šè¿‡

**é˜¶æ®µ 3: é‡‘ä¸é›€å‘å¸ƒï¼ˆ1 å‘¨ï¼‰**
```bash
# åˆå¹¶åˆ°ä¸»åˆ†æ”¯
git checkout main
git merge fix/cache-memory-leak

# æ ‡è®°ç‰ˆæœ¬
git tag v1.0.5-cache-fix

# é€šçŸ¥ç”¨æˆ·åœ¨æ–°å®éªŒä¸­ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆæœ¬
```

**å›æ»šé¢„æ¡ˆ**:
```bash
# å¦‚æœå‘ç°ä»»ä½•æ•°å€¼å·®å¼‚æˆ–æ€§èƒ½é€€åŒ–
git revert <commit-hash>
git push origin main
```

---

## è§£å†³æ–¹æ¡ˆ 2: ä¼˜åŒ– GPUâ†”CPU æ•°æ®ä¼ è¾“

### æ–¹æ¡ˆè®¾è®¡

**ç›®æ ‡**: æ¶ˆé™¤ `tolist()` è°ƒç”¨ï¼Œä½¿ç”¨çº¯ GPU æ“ä½œè¿›è¡Œåˆ†æ¡¶

**æŠ€æœ¯å®ç°**:

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/models/friction_layer.py
# ä¿®æ”¹èŒƒå›´: ç¬¬ 43-46 è¡Œ
# ============================================================

# ====== ä¿®æ”¹å‰ ======
lengths = attention_mask.sum(dim=1).to(torch.int64)
buckets: dict[int, List[int]] = defaultdict(list)
for idx, length in enumerate(lengths.tolist()):  # â† GPUâ†’CPU ä¼ è¾“
    buckets[int(length)].append(idx)

# ====== ä¿®æ”¹å ======
lengths = attention_mask.sum(dim=1).to(torch.int64)

# ä½¿ç”¨çº¯ GPU æ“ä½œè¿›è¡Œåˆ†æ¡¶ï¼ˆæ—  Python å¾ªç¯ï¼‰
unique_lengths = torch.unique(lengths)  # GPU æ“ä½œ
buckets: dict[int, torch.Tensor] = {}

for length_scalar in unique_lengths:
    # æ‰¾åˆ°æ‰€æœ‰é•¿åº¦ç­‰äº length_scalar çš„ç´¢å¼•
    # æ³¨æ„ï¼šè¿™é‡Œä»éœ€è¦ .item() æ¥è·å–æ ‡é‡ï¼Œä½†å¾ªç¯æ¬¡æ•°å¤§å¤§å‡å°‘
    length_val = int(length_scalar.item())  # ä»…å¯¹ unique é•¿åº¦è°ƒç”¨ï¼ˆé€šå¸¸ < 10 æ¬¡ï¼‰
    mask = (lengths == length_scalar)
    indices = torch.where(mask)[0]  # è¿”å› tensorï¼Œä¿æŒåœ¨ GPU
    buckets[length_val] = indices

# ====== åç»­ä»£ç é€‚é… ======
for length, indices in buckets.items():
    if length <= 1:
        continue

    # indices ç°åœ¨æ˜¯ tensor è€Œé listï¼Œéœ€è¦é€‚é…ç´¢å¼•æ“ä½œ
    seq_hidden = hidden[indices, :length].contiguous()  # â† tensor ç´¢å¼•ä»ç„¶æœ‰æ•ˆ

    if self.config.neighbor == "window":
        edges = sparse_utils.build_window_edges(
            length, radius=self.config.radius, device=hidden.device
        )
        seq_out, seq_energy = self._run_window_batch(seq_hidden, edges)
    else:
        bucket_mask = attention_mask[indices, :length]  # â† tensor ç´¢å¼•ä»ç„¶æœ‰æ•ˆ
        edges = sparse_utils.build_knn_edges_batched(
            seq_hidden, bucket_mask, k=self.config.k
        )
        seq_out, seq_energy = self._run_knn_batch(seq_hidden, edges)

    outputs[indices, :length] = seq_out  # â† tensor ç´¢å¼•ä»ç„¶æœ‰æ•ˆ
    energies[indices] = seq_energy
```

### ä¼˜åŒ–æ•ˆæœåˆ†æ

**æ€§èƒ½æå‡é¢„ä¼°**:
```
åœºæ™¯ï¼šbatch_size=32, unique_lengths=4ï¼ˆå…¸å‹æƒ…å†µï¼‰

ä¿®æ”¹å‰ï¼š
- lengths.tolist(): 32 æ¬¡ GPUâ†’CPU æ‹·è´ â†’ ~0.5ms
- Python for å¾ªç¯: 32 æ¬¡è¿­ä»£ â†’ ~0.3ms
- æ€»è®¡: ~0.8ms/batch

ä¿®æ”¹åï¼š
- torch.unique(): GPU æ“ä½œ â†’ ~0.05ms
- Python for å¾ªç¯: 4 æ¬¡è¿­ä»£ï¼ˆä»… unique é•¿åº¦ï¼‰ â†’ ~0.1ms
- .item() è°ƒç”¨: 4 æ¬¡ â†’ ~0.1ms
- æ€»è®¡: ~0.25ms/batch

åŠ é€Ÿæ¯”: 0.8 / 0.25 = 3.2xï¼ˆåœ¨è¯¥æ¨¡å—ï¼‰
æ•´ä½“è®­ç»ƒåŠ é€Ÿ: ~5-10%ï¼ˆå› ä¸ºæ­¤æ¨¡å—å æ€»æ—¶é—´ 30-50%ï¼‰
```

### éªŒè¯æ–¹æ¡ˆ

**éªŒè¯ 1: åˆ†æ¡¶ç»“æœä¸€è‡´æ€§**

```python
# tests/test_friction_layer_optimization.py
import torch
from fif_mvp.models.friction_layer import FrictionLayer

def test_bucketing_equivalence():
    """éªŒè¯ä¼˜åŒ–åçš„åˆ†æ¡¶é€»è¾‘ä¸åŸé€»è¾‘å®Œå…¨ä¸€è‡´"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 16
    max_len = 50
    attention_mask = torch.randint(0, 2, (batch_size, max_len))

    # æ¨¡æ‹ŸåŸé€»è¾‘ï¼ˆä½¿ç”¨ tolistï¼‰
    def original_bucketing(mask):
        lengths = mask.sum(dim=1).to(torch.int64)
        buckets = {}
        for idx, length in enumerate(lengths.tolist()):
            buckets.setdefault(int(length), []).append(idx)
        return buckets

    # æ–°é€»è¾‘ï¼ˆGPU ä¼˜åŒ–ï¼‰
    def optimized_bucketing(mask):
        lengths = mask.sum(dim=1).to(torch.int64)
        unique_lengths = torch.unique(lengths)
        buckets = {}
        for length_scalar in unique_lengths:
            length_val = int(length_scalar.item())
            mask_len = (lengths == length_scalar)
            indices = torch.where(mask_len)[0]
            buckets[length_val] = indices.tolist()  # è½¬ä¸º list ä¾¿äºå¯¹æ¯”
        return buckets

    # å¯¹æ¯”ç»“æœ
    original = original_bucketing(attention_mask)
    optimized = optimized_bucketing(attention_mask)

    assert original.keys() == optimized.keys(), "æ¡¶çš„æ•°é‡ä¸ä¸€è‡´"
    for length in original:
        assert sorted(original[length]) == sorted(optimized[length]), \
            f"é•¿åº¦ {length} çš„æ¡¶å†…å®¹ä¸ä¸€è‡´"
```

**éªŒè¯ 2: ç«¯åˆ°ç«¯æ•°å€¼ä¸€è‡´æ€§**

```python
def test_forward_pass_numerical_equivalence():
    """éªŒè¯å®Œæ•´å‰å‘ä¼ æ’­è¾“å‡ºä¸å˜"""
    from fif_mvp.config import FrictionConfig

    # åˆ›å»ºæ¨¡å‹
    config = FrictionConfig(neighbor="window", radius=3)
    layer = FrictionLayer(config, hidden_size=768)
    layer.eval()

    # æµ‹è¯•è¾“å…¥
    torch.manual_seed(42)
    hidden = torch.randn(8, 30, 768)
    attention_mask = torch.ones(8, 30)
    attention_mask[0, 20:] = 0  # æ¨¡æ‹Ÿä¸åŒé•¿åº¦
    attention_mask[1, 15:] = 0

    # åŠ è½½å‚è€ƒè¾“å‡ºï¼ˆç”±åŸä»£ç ç”Ÿæˆï¼‰
    reference = torch.load("tests/fixtures/friction_layer_reference.pt")

    # è¿è¡Œä¼˜åŒ–åçš„ä»£ç 
    with torch.no_grad():
        outputs, energies = layer(hidden, attention_mask)

    # éªŒè¯æ•°å€¼ä¸€è‡´æ€§ï¼ˆå…è®¸ 1e-6 æµ®ç‚¹è¯¯å·®ï¼‰
    torch.testing.assert_close(outputs, reference["outputs"], rtol=1e-6, atol=1e-6)
    torch.testing.assert_close(energies, reference["energies"], rtol=1e-6, atol=1e-6)
```

**éªŒè¯ 3: æ€§èƒ½åŸºå‡†æµ‹è¯•**

```python
# tests/benchmark_friction_layer.py
import time
import torch
from fif_mvp.models.friction_layer import FrictionLayer

def benchmark_forward_pass(num_iterations=100):
    """æµ‹é‡å‰å‘ä¼ æ’­å¹³å‡è€—æ—¶"""
    config = FrictionConfig(neighbor="window", radius=3)
    layer = FrictionLayer(config, hidden_size=768).cuda()

    # é¢„çƒ­
    hidden = torch.randn(32, 20, 768, device="cuda")
    mask = torch.ones(32, 20, device="cuda")
    for _ in range(10):
        layer(hidden, mask)

    # åŸºå‡†æµ‹è¯•
    torch.cuda.synchronize()
    start = time.perf_counter()

    for _ in range(num_iterations):
        layer(hidden, mask)

    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start

    avg_time = elapsed / num_iterations * 1000  # è½¬ä¸º ms
    print(f"å¹³å‡å‰å‘ä¼ æ’­æ—¶é—´: {avg_time:.3f} ms")
    return avg_time

if __name__ == "__main__":
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    baseline = 2.5  # msï¼ˆåŸä»£ç çš„å‚è€ƒå€¼ï¼‰
    current = benchmark_forward_pass()

    speedup = baseline / current
    print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
    assert current < baseline, "æ€§èƒ½é€€åŒ–ï¼"
```

### éƒ¨ç½²è®¡åˆ’

ä¸è§£å†³æ–¹æ¡ˆ 1 ç›¸åŒçš„ä¸‰é˜¶æ®µæµç¨‹ï¼Œå…³é”®å·®å¼‚ï¼š

**é˜¶æ®µ 1: ç”Ÿæˆå‚è€ƒæ•°æ®**
```bash
# åœ¨åº”ç”¨ä¼˜åŒ–å‰ï¼Œç”Ÿæˆå‚è€ƒè¾“å‡º
git checkout main  # åˆ‡æ¢åˆ°åŸä»£ç 
python scripts/generate_friction_layer_reference.py

# å‚è€ƒæ•°æ®åŒ…æ‹¬ï¼š
# - tests/fixtures/friction_layer_reference.pt
# - tests/fixtures/training_metrics_baseline.json
```

**é˜¶æ®µ 2: å®æ–½ä¼˜åŒ–å¹¶éªŒè¯**
```bash
git checkout -b optimize/gpu-cpu-transfer
# åº”ç”¨ä»£ç ä¿®æ”¹
pytest tests/test_friction_layer_optimization.py -v
python tests/benchmark_friction_layer.py
```

**é˜¶æ®µ 3: å®Œæ•´è®­ç»ƒéªŒè¯**
```bash
# è¿è¡Œå®Œæ•´è®­ç»ƒï¼Œç¡®ä¿æ”¶æ•›æ›²çº¿ä¸€è‡´
python -m fif_mvp.cli.run_experiment --dataset sst2 --epochs 10
python scripts/compare_training_curves.py \
    tests/fixtures/training_metrics_baseline.json \
    result/optimized/metrics.json
```

---

## è§£å†³æ–¹æ¡ˆ 3: å‘é‡åŒ–æ··æ·†çŸ©é˜µè®¡ç®—

### æ–¹æ¡ˆè®¾è®¡

**ç›®æ ‡**: ä½¿ç”¨ NumPy å‘é‡åŒ–æ“ä½œæ›¿ä»£ Python å¾ªç¯

**æŠ€æœ¯å®ç°**:

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/train/metrics.py
# ä¿®æ”¹èŒƒå›´: ç¬¬ 36-44 è¡Œ
# ============================================================

# ====== ä¿®æ”¹å‰ ======
def confusion_matrix(
    labels: np.ndarray, preds: np.ndarray, num_labels: int
) -> np.ndarray:
    """Return counts matrix."""
    matrix = np.zeros((num_labels, num_labels), dtype=int)
    for y_true, y_pred in zip(labels, preds):  # â† Python å¾ªç¯
        matrix[y_true, y_pred] += 1
    return matrix

# ====== ä¿®æ”¹å ======
def confusion_matrix(
    labels: np.ndarray, preds: np.ndarray, num_labels: int
) -> np.ndarray:
    """è¿”å›æ··æ·†çŸ©é˜µï¼ˆå‘é‡åŒ–å®ç°ï¼‰ã€‚

    å˜æ›´è¯´æ˜ï¼š
    - ä½¿ç”¨ np.bincount å‘é‡åŒ–è®¡ç®—ï¼Œæ¶ˆé™¤ Python å¾ªç¯
    - æ•°å€¼ç»“æœä¸åŸå®ç°é€ä½ä¸€è‡´
    - æ€§èƒ½æå‡ï¼šO(n) Python å¾ªç¯ â†’ O(n) C çº§åˆ«æ“ä½œ

    Args:
        labels: çœŸå®æ ‡ç­¾ï¼Œshape (N,)
        preds: é¢„æµ‹æ ‡ç­¾ï¼Œshape (N,)
        num_labels: ç±»åˆ«æ€»æ•°

    Returns:
        æ··æ·†çŸ©é˜µï¼Œshape (num_labels, num_labels)
        matrix[i, j] = çœŸå®ç±»åˆ« i è¢«é¢„æµ‹ä¸ºç±»åˆ« j çš„æ¬¡æ•°
    """
    # è¾“å…¥éªŒè¯ï¼ˆæ–°å¢ï¼Œæé«˜å¥å£®æ€§ï¼‰
    assert labels.shape == preds.shape, "æ ‡ç­¾å’Œé¢„æµ‹å½¢çŠ¶ä¸åŒ¹é…"
    assert labels.min() >= 0 and labels.max() < num_labels, "æ ‡ç­¾è¶…å‡ºèŒƒå›´"
    assert preds.min() >= 0 and preds.max() < num_labels, "é¢„æµ‹è¶…å‡ºèŒƒå›´"

    # å‘é‡åŒ–è®¡ç®—
    # åŸç†ï¼šå°† (i, j) äºŒç»´ç´¢å¼•ç¼–ç ä¸ºä¸€ç»´ index = i * num_labels + j
    indices = labels * num_labels + preds
    flat_counts = np.bincount(indices, minlength=num_labels ** 2)
    matrix = flat_counts.reshape(num_labels, num_labels)

    return matrix
```

### æ€§èƒ½åˆ†æ

```
åœºæ™¯ï¼šSNLI æµ‹è¯•é›†ï¼Œ10,000 æ ·æœ¬ï¼Œ3 ç±»åˆ«

ä¿®æ”¹å‰ï¼š
- Python for å¾ªç¯: 10,000 æ¬¡è¿­ä»£
- æ¯æ¬¡è¿­ä»£: æ•°ç»„ç´¢å¼• + æ•´æ•°åŠ æ³•
- æ€»è€—æ—¶: ~50ms

ä¿®æ”¹åï¼š
- np.bincount: C çº§åˆ«å‘é‡åŒ–æ“ä½œ
- æ€»è€—æ—¶: ~2ms

åŠ é€Ÿæ¯”: 50 / 2 = 25x
```

### éªŒè¯æ–¹æ¡ˆ

```python
# tests/test_metrics_optimization.py
import numpy as np
from fif_mvp.train.metrics import confusion_matrix

def test_confusion_matrix_correctness():
    """éªŒè¯å‘é‡åŒ–å®ç°ä¸åŸé€»è¾‘å®Œå…¨ä¸€è‡´"""
    # æµ‹è¯•ç”¨ä¾‹ 1: åŸºæœ¬æ¡ˆä¾‹
    labels = np.array([0, 1, 2, 0, 1, 2])
    preds = np.array([0, 1, 1, 0, 2, 2])
    num_labels = 3

    result = confusion_matrix(labels, preds, num_labels)

    # æ‰‹åŠ¨è®¡ç®—çš„æœŸæœ›ç»“æœ
    expected = np.array([
        [2, 0, 0],  # çœŸå® 0: é¢„æµ‹ä¸º 0 ä¸¤æ¬¡
        [0, 1, 1],  # çœŸå® 1: é¢„æµ‹ä¸º 1 ä¸€æ¬¡ï¼Œé¢„æµ‹ä¸º 2 ä¸€æ¬¡
        [0, 1, 1],  # çœŸå® 2: é¢„æµ‹ä¸º 1 ä¸€æ¬¡ï¼Œé¢„æµ‹ä¸º 2 ä¸€æ¬¡
    ])

    np.testing.assert_array_equal(result, expected)

def test_confusion_matrix_large_scale():
    """æµ‹è¯•å¤§è§„æ¨¡æ•°æ®"""
    np.random.seed(42)
    labels = np.random.randint(0, 3, size=10000)
    preds = np.random.randint(0, 3, size=10000)

    # ä½¿ç”¨åŸå§‹å¾ªç¯å®ç°ä½œä¸ºå‚è€ƒ
    def reference_implementation(labels, preds, num_labels):
        matrix = np.zeros((num_labels, num_labels), dtype=int)
        for y_true, y_pred in zip(labels, preds):
            matrix[y_true, y_pred] += 1
        return matrix

    result = confusion_matrix(labels, preds, 3)
    expected = reference_implementation(labels, preds, 3)

    np.testing.assert_array_equal(result, expected)

def test_confusion_matrix_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    # ç©ºè¾“å…¥
    result = confusion_matrix(np.array([]), np.array([]), 2)
    assert result.shape == (2, 2)
    assert result.sum() == 0

    # å•ç±»åˆ«
    result = confusion_matrix(np.array([0, 0, 0]), np.array([0, 0, 0]), 1)
    np.testing.assert_array_equal(result, np.array([[3]]))
```

### éƒ¨ç½²è®¡åˆ’

**ä½é£é™©å¿«é€Ÿéƒ¨ç½²**:
```bash
# æ­¤ä¼˜åŒ–ä»…å½±å“è¯„ä¼°é˜¶æ®µï¼Œä¸å½±å“è®­ç»ƒ
# å¯ä»¥ç›´æ¥åˆå¹¶ï¼Œæ— éœ€é‡‘ä¸é›€å‘å¸ƒ

git checkout -b optimize/vectorize-metrics
# åº”ç”¨ä¿®æ”¹
pytest tests/test_metrics_optimization.py -v
git commit -m "optimize: vectorize confusion matrix (25x speedup)"
git push origin optimize/vectorize-metrics
# åˆ›å»º PR å¹¶åˆå¹¶
```

---

## è§£å†³æ–¹æ¡ˆ 4: å›ºå®šä¾èµ–ç‰ˆæœ¬

### æ–¹æ¡ˆè®¾è®¡

**ç›®æ ‡**: ç¡®ä¿ä»»ä½•äººåœ¨ä»»ä½•æ—¶é—´ç‚¹å®‰è£…çš„ä¾èµ–ç‰ˆæœ¬å®Œå…¨ä¸€è‡´

**æŠ€æœ¯å®ç°**:

```bash
# ============================================================
# æ–‡ä»¶: requirements.txt
# ä¿®æ”¹èŒƒå›´: å…¨éƒ¨å†…å®¹
# ============================================================

# ====== ä¿®æ”¹å‰ ======
torch>=2.2
numpy>=1.26
pandas>=2.2
tqdm>=4.66
scikit-learn>=1.5
datasets>=2.20
transformers>=4.44

# ====== ä¿®æ”¹å ======
# FIF-A ä¾èµ–é”å®šç‰ˆæœ¬
# ç”Ÿæˆæ—¶é—´: 2025-12-02
# Python ç‰ˆæœ¬: 3.10+
# CUDA ç‰ˆæœ¬: 11.8+ (torch 2.2.0 ç¼–è¯‘ç‰ˆæœ¬)

# æ ¸å¿ƒæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆç‰ˆæœ¬ä¸¥æ ¼é”å®šï¼‰
torch==2.2.0
numpy==1.26.4
transformers==4.44.0
datasets==2.20.0

# æ•°æ®å¤„ç†ï¼ˆæ¬¡è¦ä¾èµ–ï¼Œå…è®¸è¡¥ä¸ç‰ˆæœ¬æ›´æ–°ï¼‰
pandas==2.2.0
scikit-learn==1.5.0

# å·¥å…·åº“ï¼ˆå¯ä»¥çµæ´»æ›´æ–°ï¼‰
tqdm==4.66.0

# ============================================================
# æ–°å¢æ–‡ä»¶: requirements-dev.txt
# ç”¨é€”: å¼€å‘ç¯å¢ƒé¢å¤–ä¾èµ–
# ============================================================
-r requirements.txt  # ç»§æ‰¿ç”Ÿäº§ä¾èµ–

# æµ‹è¯•å·¥å…·
pytest==8.0.0
pytest-cov==4.1.0
pytest-xdist==3.5.0  # å¹¶è¡Œæµ‹è¯•

# ä»£ç è´¨é‡
black==24.1.0
isort==5.13.0
flake8==7.0.0
mypy==1.8.0

# æ–‡æ¡£ç”Ÿæˆ
sphinx==7.2.0
sphinx-rtd-theme==2.0.0

# æ€§èƒ½åˆ†æ
line-profiler==4.1.0
memory-profiler==0.61.0

# ============================================================
# æ–°å¢æ–‡ä»¶: requirements-lock.txt
# ç”¨é€”: å®Œæ•´ä¾èµ–æ ‘é”å®šï¼ˆåŒ…æ‹¬ä¼ é€’ä¾èµ–ï¼‰
# ç”Ÿæˆæ–¹å¼: pip freeze > requirements-lock.txt
# ============================================================
torch==2.2.0
numpy==1.26.4
transformers==4.44.0
datasets==2.20.0
# ... åŒ…æ‹¬æ‰€æœ‰ä¼ é€’ä¾èµ–çš„ç²¾ç¡®ç‰ˆæœ¬
filelock==3.13.1
fsspec==2024.2.0
huggingface-hub==0.20.3
# ... ç­‰ç­‰
```

### ç‰ˆæœ¬é€‰æ‹©ç­–ç•¥

**ç‰ˆæœ¬é”å®šåŸåˆ™**:

1. **ä¸¥æ ¼é”å®š** (`==`)ï¼š
   - `torch`: æ ¸å¿ƒä¾èµ–ï¼Œç‰ˆæœ¬å˜åŒ–å¯èƒ½å½±å“æ•°å€¼ç¨³å®šæ€§
   - `transformers`: API å˜åŒ–é¢‘ç¹ï¼Œå¿…é¡»é”å®š
   - `datasets`: æ•°æ®åŠ è½½é€»è¾‘ä¾èµ–ç‰¹å®šç‰ˆæœ¬

2. **è¡¥ä¸ç‰ˆæœ¬å…è®¸** (`~=2.2.0` ç­‰ä»·äº `>=2.2.0, <2.3.0`)ï¼š
   - `pandas`: ä»…ç”¨äºç»“æœä¿å­˜ï¼Œè¡¥ä¸æ›´æ–°å®‰å…¨
   - `scikit-learn`: ä»…ç”¨äºæŒ‡æ ‡è®¡ç®—ï¼Œå°ç‰ˆæœ¬å…¼å®¹

3. **çµæ´»æ›´æ–°** (ä¿æŒ `==` ä½†å®šæœŸå®¡æŸ¥)ï¼š
   - `tqdm`: çº¯æ˜¾ç¤ºåº“ï¼Œå½±å“å°

### éªŒè¯æ–¹æ¡ˆ

**æµ‹è¯• 1: ä¾èµ–å®‰è£…å¯é‡å¤æ€§**

```bash
# tests/test_reproducibility.sh
#!/bin/bash

# åˆ›å»ºå¹²å‡€çš„è™šæ‹Ÿç¯å¢ƒ
python -m venv /tmp/fif_test_env
source /tmp/fif_test_env/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯ç‰ˆæœ¬
python -c "import torch; assert torch.__version__ == '2.2.0', 'torch ç‰ˆæœ¬ä¸åŒ¹é…'"
python -c "import transformers; assert transformers.__version__ == '4.44.0'"
python -c "import datasets; assert datasets.__version__ == '2.20.0'"

echo "âœ“ ä¾èµ–ç‰ˆæœ¬éªŒè¯é€šè¿‡"

# æ¸…ç†
deactivate
rm -rf /tmp/fif_test_env
```

**æµ‹è¯• 2: å¤šç¯å¢ƒéªŒè¯**

```yaml
# .github/workflows/test-dependencies.yml
name: Test Dependency Lock

on: [push, pull_request]

jobs:
  test-installation:
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ['3.10', '3.11']

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Verify versions
        run: |
          python -c "import torch; print(f'torch: {torch.__version__}')"
          python -c "import transformers; print(f'transformers: {transformers.__version__}')"
          python tests/verify_dependency_versions.py

      - name: Run smoke test
        run: |
          python -m fif_mvp.cli.run_experiment --help
```

### è¿ç§»è®¡åˆ’

**é˜¶æ®µ 1: ç”Ÿæˆé”å®šç‰ˆæœ¬ï¼ˆ1 å¤©ï¼‰**

```bash
# åœ¨å½“å‰ç¨³å®šç¯å¢ƒä¸­ç”Ÿæˆé”å®šæ–‡ä»¶
pip freeze > requirements-lock-candidate.txt

# æ‰‹åŠ¨å®¡æŸ¥å¹¶æ¸…ç†ä¸å¿…è¦çš„ä¾èµ–
# æå–æ ¸å¿ƒä¾èµ–åˆ° requirements.txt

# æäº¤å˜æ›´
git add requirements.txt requirements-dev.txt requirements-lock.txt
git commit -m "deps: lock dependency versions for reproducibility"
```

**é˜¶æ®µ 2: æ›´æ–°æ–‡æ¡£ï¼ˆ1 å¤©ï¼‰**

```markdown
# åœ¨ README.md ä¸­æ·»åŠ è¯´æ˜

## å®‰è£…

### ç”Ÿäº§ç¯å¢ƒï¼ˆæ¨èï¼‰
```bash
pip install -r requirements.txt
```

### å¼€å‘ç¯å¢ƒ
```bash
pip install -r requirements-dev.txt
```

### å®Œå…¨é”å®šç¯å¢ƒï¼ˆä¿è¯é€ä½ä¸€è‡´ï¼‰
```bash
pip install -r requirements-lock.txt
```

## ä¾èµ–æ›´æ–°ç­–ç•¥

**é‡è¦**: ä¸è¦éšæ„æ›´æ–°æ ¸å¿ƒä¾èµ–ï¼ˆtorch, transformers, datasetsï¼‰

å¦‚æœå¿…é¡»æ›´æ–°ï¼š
1. åˆ›å»ºæ–°åˆ†æ”¯
2. æ›´æ–°ä¾èµ–ç‰ˆæœ¬
3. è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
4. éªŒè¯è®­ç»ƒç»“æœæ•°å€¼ä¸€è‡´æ€§ï¼ˆå…è®¸ 1e-5 è¯¯å·®ï¼‰
5. æ›´æ–° requirements-lock.txt
6. è®°å½•å˜æ›´æ—¥å¿—
```

**é˜¶æ®µ 3: CI/CD é›†æˆï¼ˆ3 å¤©ï¼‰**

```bash
# æ·»åŠ ä¾èµ–éªŒè¯åˆ° CI æµæ°´çº¿
# æ¯æ¬¡ PR è‡ªåŠ¨æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬
# é˜²æ­¢æ„å¤–æäº¤ >= ç‰ˆæœ¬å·
```

---

## è§£å†³æ–¹æ¡ˆ 5: æ”¹è¿›å¼‚å¸¸å¤„ç†

### æ–¹æ¡ˆè®¾è®¡

**ç›®æ ‡**: ç§»é™¤é™é»˜å¤±è´¥ï¼Œä½¿ç”¨å…·ä½“å¼‚å¸¸ç±»å‹ï¼Œæ·»åŠ æ—¥å¿—è®°å½•

**æŠ€æœ¯å®ç°**:

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/cli/run_experiment.py
# ä¿®æ”¹èŒƒå›´: ç¬¬ 468-473 è¡Œ å’Œ ç¬¬ 320 è¡Œ
# ============================================================

# ====== ä¿®æ”¹å‰ (ä½ç½® 1: é™é»˜å¤±è´¥) ======
try:
    if hasattr(torch, "set_float32_matmul_precision") and major >= 8:
        torch.set_float32_matmul_precision("high")
except Exception:
    pass  # â† é—®é¢˜ï¼šå®Œå…¨å¿½ç•¥é”™è¯¯

# ====== ä¿®æ”¹å (ä½ç½® 1) ======
import logging
logger = logging.getLogger(__name__)

try:
    if hasattr(torch, "set_float32_matmul_precision") and major >= 8:
        torch.set_float32_matmul_precision("high")
        logger.info("Set float32 matmul precision to 'high' (Ampere+)")
except (AttributeError, RuntimeError) as exc:
    # ä»…æ•è·é¢„æœŸçš„å¼‚å¸¸ç±»å‹
    logger.warning(
        "Failed to set float32 matmul precision (non-critical): %s",
        exc,
        exc_info=False  # ä¸æ‰“å°å®Œæ•´å †æ ˆï¼ˆéä¸¥é‡é”™è¯¯ï¼‰
    )
    # ä¸ä¸­æ–­æ‰§è¡Œï¼Œå› ä¸ºè¿™æ˜¯æ€§èƒ½ä¼˜åŒ–è€Œéå¿…éœ€åŠŸèƒ½
except Exception as exc:
    # æ•è·æ„å¤–å¼‚å¸¸ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
    logger.error(
        "Unexpected error in matmul precision setup: %s",
        exc,
        exc_info=True  # æ‰“å°å®Œæ•´å †æ ˆä»¥ä¾¿è°ƒè¯•
    )
    # ä»ç„¶ä¸ä¸­æ–­æ‰§è¡Œï¼Œä½†ç°åœ¨æœ‰æ—¥å¿—å¯è¿½è¸ª

# ====== ä¿®æ”¹å‰ (ä½ç½® 2: è¿‡äºå®½æ³›çš„å¼‚å¸¸) ======
try:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
except Exception as exc:  # â† é—®é¢˜ï¼šæ•è·æ‰€æœ‰å¼‚å¸¸ï¼ˆåŒ…æ‹¬ KeyboardInterruptï¼‰
    raise RuntimeError("CUDA init failed") from exc

# ====== ä¿®æ”¹å (ä½ç½® 2) ======
try:
    if torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info("Using CUDA device: %s", torch.cuda.get_device_name(0))
    else:
        device = torch.device("cpu")
        logger.warning("CUDA not available, using CPU (training will be slow)")
except (RuntimeError, AssertionError) as exc:
    # RuntimeError: CUDA é©±åŠ¨é—®é¢˜
    # AssertionError: torch.cuda å†…éƒ¨æ–­è¨€å¤±è´¥
    logger.error("CUDA initialization failed: %s", exc)
    logger.info("Falling back to CPU")
    device = torch.device("cpu")
except Exception as exc:
    # æ•è·å…¶ä»–æ„å¤–å¼‚å¸¸ï¼Œä½†è®°å½•è¯¦ç»†ä¿¡æ¯
    logger.critical(
        "Unexpected error during device initialization: %s",
        exc,
        exc_info=True
    )
    # é‡æ–°æŠ›å‡ºï¼Œå› ä¸ºè¿™æ˜¯æ— æ³•æ¢å¤çš„é”™è¯¯
    raise RuntimeError("Device initialization failed critically") from exc
```

### å¼‚å¸¸å¤„ç†æœ€ä½³å®è·µæ€»ç»“

**åŸåˆ™**:
1. **å…·ä½“åŒ–å¼‚å¸¸ç±»å‹**: ä¸ä½¿ç”¨è£¸ `except Exception`
2. **è®°å½•æ—¥å¿—**: æ‰€æœ‰å¼‚å¸¸éƒ½åº”è®°å½•ï¼Œä¾¿äºäº‹ååˆ†æ
3. **åŒºåˆ†ä¸¥é‡æ€§**:
   - å¯æ¢å¤é”™è¯¯ï¼ˆwarningï¼‰ï¼šç»§ç»­æ‰§è¡Œ
   - ä¸¥é‡é”™è¯¯ï¼ˆerror/criticalï¼‰ï¼šä¸­æ–­æ‰§è¡Œ
4. **æä¾›ä¸Šä¸‹æ–‡**: å¼‚å¸¸æ¶ˆæ¯åº”åŒ…å«è¶³å¤Ÿä¿¡æ¯å®šä½é—®é¢˜

**å®æ–½æ¸…å•**:

```python
# ============================================================
# æ–°å¢æ–‡ä»¶: fif_mvp/utils/error_handling.py
# ç”¨é€”: é›†ä¸­ç®¡ç†è‡ªå®šä¹‰å¼‚å¸¸å’Œé”™è¯¯å¤„ç†å·¥å…·
# ============================================================

import logging
from typing import Optional, Type, Tuple

logger = logging.getLogger(__name__)


class FIFError(Exception):
    """FIF-A é¡¹ç›®çš„åŸºç¡€å¼‚å¸¸ç±»"""
    pass


class DeviceInitError(FIFError):
    """è®¾å¤‡åˆå§‹åŒ–å¤±è´¥"""
    pass


class DataLoadError(FIFError):
    """æ•°æ®åŠ è½½å¤±è´¥"""
    pass


class ModelConfigError(FIFError):
    """æ¨¡å‹é…ç½®é”™è¯¯"""
    pass


def safe_execute(
    func,
    *args,
    expected_exceptions: Tuple[Type[Exception], ...] = (),
    fallback_value=None,
    error_message: Optional[str] = None,
    log_level: str = "warning",
    **kwargs
):
    """å®‰å…¨æ‰§è¡Œå‡½æ•°ï¼Œæ•è·é¢„æœŸå¼‚å¸¸å¹¶è®°å½•æ—¥å¿—ã€‚

    ç¤ºä¾‹ç”¨æ³•ï¼š
        result = safe_execute(
            torch.set_float32_matmul_precision,
            "high",
            expected_exceptions=(AttributeError, RuntimeError),
            fallback_value=None,
            error_message="Failed to set matmul precision",
            log_level="warning"
        )

    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        *args: å‡½æ•°å‚æ•°
        expected_exceptions: é¢„æœŸçš„å¼‚å¸¸ç±»å‹
        fallback_value: å¼‚å¸¸å‘ç”Ÿæ—¶çš„è¿”å›å€¼
        error_message: è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯
        log_level: æ—¥å¿—çº§åˆ« (debug/info/warning/error/critical)
        **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°

    Returns:
        å‡½æ•°è¿”å›å€¼ï¼Œæˆ–å¼‚å¸¸æ—¶çš„ fallback_value
    """
    try:
        return func(*args, **kwargs)
    except expected_exceptions as exc:
        msg = error_message or f"{func.__name__} failed"
        getattr(logger, log_level)(f"{msg}: {exc}")
        return fallback_value
    except Exception as exc:
        # æ„å¤–å¼‚å¸¸ï¼Œè®°å½•è¯¦ç»†å †æ ˆ
        msg = error_message or f"{func.__name__} failed unexpectedly"
        logger.error(f"{msg}: {exc}", exc_info=True)
        raise
```

### ä½¿ç”¨ç¤ºä¾‹

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/cli/run_experiment.py
# ä½¿ç”¨æ–°çš„é”™è¯¯å¤„ç†å·¥å…·
# ============================================================

from fif_mvp.utils.error_handling import safe_execute, DeviceInitError

# æ›¿ä»£åŸæ¥çš„ try-except å—
safe_execute(
    torch.set_float32_matmul_precision,
    "high",
    expected_exceptions=(AttributeError, RuntimeError),
    error_message="Failed to set matmul precision (non-critical)",
    log_level="warning"
)

# è®¾å¤‡åˆå§‹åŒ–ï¼ˆå…³é”®æ“ä½œï¼Œå¤±è´¥åº”æŠ›å‡ºå¼‚å¸¸ï¼‰
if not torch.cuda.is_available():
    logger.warning("CUDA not available, using CPU")
    device = torch.device("cpu")
else:
    try:
        device = torch.device("cuda")
        torch.cuda.init()  # æ˜¾å¼åˆå§‹åŒ–ï¼Œææ—©å‘ç°é—®é¢˜
        logger.info("CUDA initialized: %s", torch.cuda.get_device_name(0))
    except (RuntimeError, AssertionError) as exc:
        raise DeviceInitError(
            f"CUDA initialization failed: {exc}. "
            "Check CUDA drivers and GPU availability."
        ) from exc
```

### éªŒè¯æ–¹æ¡ˆ

```python
# tests/test_error_handling.py
import pytest
import logging
from fif_mvp.utils.error_handling import safe_execute, FIFError

def test_safe_execute_success():
    """æµ‹è¯•æ­£å¸¸æ‰§è¡Œ"""
    result = safe_execute(lambda x: x * 2, 5)
    assert result == 10

def test_safe_execute_expected_exception():
    """æµ‹è¯•æ•è·é¢„æœŸå¼‚å¸¸"""
    def failing_func():
        raise ValueError("expected error")

    result = safe_execute(
        failing_func,
        expected_exceptions=(ValueError,),
        fallback_value="fallback"
    )
    assert result == "fallback"

def test_safe_execute_unexpected_exception():
    """æµ‹è¯•æ„å¤–å¼‚å¸¸ä¼šé‡æ–°æŠ›å‡º"""
    def failing_func():
        raise TypeError("unexpected error")

    with pytest.raises(TypeError):
        safe_execute(
            failing_func,
            expected_exceptions=(ValueError,),  # ä»…é¢„æœŸ ValueError
        )

def test_logging_on_exception(caplog):
    """æµ‹è¯•å¼‚å¸¸è¢«æ­£ç¡®è®°å½•"""
    def failing_func():
        raise ValueError("test error")

    with caplog.at_level(logging.WARNING):
        safe_execute(
            failing_func,
            expected_exceptions=(ValueError,),
            error_message="Custom error message",
            log_level="warning"
        )

    assert "Custom error message" in caplog.text
    assert "test error" in caplog.text
```

### éƒ¨ç½²è®¡åˆ’

**é˜¶æ®µ 1: åˆ›å»ºé”™è¯¯å¤„ç†å·¥å…·ï¼ˆ1 å¤©ï¼‰**
- å®ç° `utils/error_handling.py`
- ç¼–å†™å•å…ƒæµ‹è¯•

**é˜¶æ®µ 2: æ¸è¿›å¼è¿ç§»ï¼ˆ1 å‘¨ï¼‰**
```bash
# ä¼˜å…ˆä¿®å¤ P0 çº§é™é»˜å¤±è´¥
1. run_experiment.py:472-473
2. run_experiment.py:320

# ç„¶åä¿®å¤å…¶ä»–å®½æ³›å¼‚å¸¸æ•è·
3. train/loop.py ä¸­çš„å¼‚å¸¸å¤„ç†
4. data/ æ¨¡å—ä¸­çš„å¼‚å¸¸å¤„ç†
```

**é˜¶æ®µ 3: æ–‡æ¡£æ›´æ–°**
```markdown
# åœ¨å¼€å‘è€…æ–‡æ¡£ä¸­æ·»åŠ å¼‚å¸¸å¤„ç†æŒ‡å—

## å¼‚å¸¸å¤„ç†æœ€ä½³å®è·µ

1. æ°¸è¿œä¸è¦ä½¿ç”¨è£¸ `except:` æˆ– `except Exception:` è€Œä¸è®°å½•æ—¥å¿—
2. ä¼˜å…ˆä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹
3. ä½¿ç”¨ `safe_execute` å·¥å…·å¤„ç†éå…³é”®æ“ä½œ
4. å…³é”®æ“ä½œå¤±è´¥åº”æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸ï¼ˆç»§æ‰¿ FIFErrorï¼‰
```

---

## è§£å†³æ–¹æ¡ˆ 6-10: ä»£ç é‡æ„ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰

### è§£å†³æ–¹æ¡ˆ 6: æ‹†åˆ†è¶…é•¿å‡½æ•°

**é—®é¢˜**: `run_experiment.py::_run_cli()` 193 è¡Œï¼ŒèŒè´£è¿‡å¤š

**é‡æ„æ–¹æ¡ˆ**:

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/cli/run_experiment.py
# é‡æ„ç­–ç•¥: æå–å­å‡½æ•°ï¼Œä¿æŒä¸»æµç¨‹æ¸…æ™°
# ============================================================

# ====== ä¿®æ”¹å‰ (ç®€åŒ–ç¤ºæ„) ======
def _run_cli(args):
    # 193 è¡Œä»£ç ï¼ŒåŒ…å«ï¼š
    # 1. ç›®å½•åˆ›å»º (10 è¡Œ)
    # 2. è®¾å¤‡åˆå§‹åŒ– (20 è¡Œ)
    # 3. éšæœºç§å­è®¾ç½® (15 è¡Œ)
    # 4. æ•°æ®åŠ è½½ (25 è¡Œ)
    # 5. æ¨¡å‹åˆ›å»º (30 è¡Œ)
    # 6. ä¼˜åŒ–å™¨åˆ›å»º (20 è¡Œ)
    # 7. è®­ç»ƒå¾ªç¯ (50 è¡Œ)
    # 8. ç»“æœä¿å­˜ (23 è¡Œ)
    pass

# ====== ä¿®æ”¹å ======

# --- å­å‡½æ•° 1: ç›®å½•ç®¡ç† ---
def _setup_directories(args) -> Path:
    """åˆ›å»ºå¹¶éªŒè¯è¾“å‡ºç›®å½•ã€‚

    Returns:
        éªŒè¯åçš„ç»“æœç›®å½•è·¯å¾„

    Raises:
        ValueError: å¦‚æœ save_dir ä¸åœ¨ ./result ç›®å½•ä¸‹
    """
    base_result = Path(args.save_dir).expanduser().resolve()
    expected_root = (Path.cwd() / "result").resolve()

    if expected_root not in base_result.parents and base_result != expected_root:
        raise ValueError(
            f"save_dir must be within ./result, got {base_result}"
        )

    base_result.mkdir(parents=True, exist_ok=True)
    logger.info("Results will be saved to: %s", base_result)
    return base_result

# --- å­å‡½æ•° 2: è®¾å¤‡åˆå§‹åŒ– ---
def _initialize_device(args) -> Tuple[torch.device, Optional[torch.cuda.amp.GradScaler]]:
    """åˆå§‹åŒ–è®­ç»ƒè®¾å¤‡å’Œæ··åˆç²¾åº¦ scalerã€‚

    Returns:
        (device, scaler) å…ƒç»„
        scaler ä¸º None è¡¨ç¤ºä¸ä½¿ç”¨ AMP
    """
    # è®¾å¤‡é€‰æ‹©é€»è¾‘
    if torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info("Using CUDA: %s", torch.cuda.get_device_name(0))
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("Using Apple MPS")
    else:
        device = torch.device("cpu")
        logger.warning("Using CPU (training will be slow)")

    # AMP scaler åˆ›å»º
    scaler = None
    if args.use_amp and device.type == "cuda":
        scaler = torch.cuda.amp.GradScaler()
        logger.info("AMP enabled with GradScaler")

    return device, scaler

# --- å­å‡½æ•° 3: æ•°æ®åŠ è½½ ---
def _load_data(args, tokenizer):
    """åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€‚

    Returns:
        (train_loader, val_loader, num_labels) å…ƒç»„
    """
    logger.info("Loading dataset: %s", args.dataset)

    if args.dataset == "sst2":
        from fif_mvp.data import get_sst2_loaders
        train_loader, val_loader = get_sst2_loaders(
            tokenizer=tokenizer,
            batch_size=args.batch_size,
            max_length=args.max_length,
            noise_level=args.noise_level,
            num_workers=args.num_workers,
        )
        num_labels = 2
    elif args.dataset == "snli":
        from fif_mvp.data import get_snli_loaders
        train_loader, val_loader = get_snli_loaders(
            tokenizer=tokenizer,
            batch_size=args.batch_size,
            max_length=args.max_length,
            noise_level=args.noise_level,
            num_workers=args.num_workers,
        )
        num_labels = 3
    else:
        raise ValueError(f"Unknown dataset: {args.dataset}")

    logger.info(
        "Loaded %d train batches, %d val batches",
        len(train_loader),
        len(val_loader)
    )
    return train_loader, val_loader, num_labels

# --- å­å‡½æ•° 4: æ¨¡å‹åˆ›å»º ---
def _create_model(args, num_labels: int, device: torch.device):
    """åˆ›å»ºå¹¶åˆå§‹åŒ–æ¨¡å‹ã€‚

    Returns:
        model (å·²ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡)
    """
    from fif_mvp.models import create_model

    model = create_model(
        model_type=args.model_type,
        num_labels=num_labels,
        # ... å…¶ä»–é…ç½®
    )

    model = model.to(device)
    logger.info(
        "Created %s model with %d parameters",
        args.model_type,
        sum(p.numel() for p in model.parameters())
    )
    return model

# --- å­å‡½æ•° 5: ä¼˜åŒ–å™¨åˆ›å»º ---
def _create_optimizer(model, args):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

    Returns:
        (optimizer, scheduler) å…ƒç»„
    """
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )

    # ç®€åŒ–çš„è°ƒåº¦å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    scheduler = None
    if args.use_scheduler:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=args.epochs
        )

    return optimizer, scheduler

# --- ä¸»å‡½æ•° (é‡æ„åä»… 50 è¡Œ) ---
def _run_cli(args):
    """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ˆé‡æ„åï¼‰ã€‚

    æ­¤å‡½æ•°ç°åœ¨ä»…è´Ÿè´£ç¼–æ’å­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡ç”±ç‹¬ç«‹å‡½æ•°å®ç°ã€‚
    """
    # 1. è®¾ç½®è¾“å‡ºç›®å½•
    result_dir = _setup_directories(args)

    # 2. è®¾ç½®éšæœºç§å­
    from fif_mvp.utils.seed import set_seed
    set_seed(args.seed)

    # 3. åˆå§‹åŒ–è®¾å¤‡
    device, scaler = _initialize_device(args)

    # 4. åŠ è½½ tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.encoder)

    # 5. åŠ è½½æ•°æ®
    train_loader, val_loader, num_labels = _load_data(args, tokenizer)

    # 6. åˆ›å»ºæ¨¡å‹
    model = _create_model(args, num_labels, device)

    # 7. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer, scheduler = _create_optimizer(model, args)

    # 8. è®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨å·²æœ‰çš„ TrainLoop ç±»ï¼‰
    from fif_mvp.train import TrainLoop
    loop = TrainLoop(
        model=model,
        optimizer=optimizer,
        device=device,
        scaler=scaler,
        # ... å…¶ä»–å‚æ•°
    )

    metrics = loop.run(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=args.epochs
    )

    # 9. ä¿å­˜ç»“æœ
    _save_results(metrics, result_dir, args)

    logger.info("Training complete!")
    return metrics

# --- å­å‡½æ•° 6: ç»“æœä¿å­˜ ---
def _save_results(metrics: dict, result_dir: Path, args):
    """ä¿å­˜è®­ç»ƒç»“æœå’Œé…ç½®ã€‚"""
    import json

    # ä¿å­˜æŒ‡æ ‡
    with open(result_dir / "metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)

    # ä¿å­˜é…ç½®
    config_dict = vars(args)
    with open(result_dir / "config.json", "w") as f:
        json.dump(config_dict, f, indent=2)

    logger.info("Results saved to %s", result_dir)
```

**é‡æ„æ•ˆæœ**:
- ä¸»å‡½æ•°ä» 193 è¡Œç¼©å‡åˆ° 50 è¡Œ
- æ¯ä¸ªå­å‡½æ•°èŒè´£å•ä¸€ï¼Œå¯ç‹¬ç«‹æµ‹è¯•
- ä»£ç å¯è¯»æ€§å¤§å¹…æå‡

**æµ‹è¯•ç­–ç•¥**:
```python
# tests/test_cli_refactoring.py

def test_setup_directories_valid():
    """æµ‹è¯•ç›®å½•åˆ›å»º"""
    args = Mock(save_dir="./result/test")
    result = _setup_directories(args)
    assert result.exists()

def test_initialize_device_cuda():
    """æµ‹è¯• CUDA è®¾å¤‡åˆå§‹åŒ–"""
    if not torch.cuda.is_available():
        pytest.skip("CUDA not available")

    args = Mock(use_amp=True)
    device, scaler = _initialize_device(args)
    assert device.type == "cuda"
    assert isinstance(scaler, torch.cuda.amp.GradScaler)

# ... æ¯ä¸ªå­å‡½æ•°éƒ½æœ‰ç‹¬ç«‹æµ‹è¯•
```

---

### è§£å†³æ–¹æ¡ˆ 7: æå–å…±äº«æ•°æ®åŠ è½½é€»è¾‘

**é—®é¢˜**: `sst2.py` å’Œ `snli.py` ä¸­ DataLoader åˆ›å»ºä»£ç é‡å¤

**é‡æ„æ–¹æ¡ˆ**:

```python
# ============================================================
# æ–°å¢æ–‡ä»¶: fif_mvp/data/common.py
# ç”¨é€”: å…±äº«çš„æ•°æ®åŠ è½½å·¥å…·å‡½æ•°
# ============================================================

from typing import Optional, Callable
import os
import torch
from torch.utils.data import DataLoader
from transformers import PreTrainedTokenizer

def create_dataloader(
    dataset,
    tokenizer: PreTrainedTokenizer,
    batch_size: int,
    max_length: int,
    shuffle: bool = False,
    num_workers: Optional[int] = None,
    collate_fn: Optional[Callable] = None,
) -> DataLoader:
    """åˆ›å»º DataLoader çš„ç»Ÿä¸€å·¥å‚å‡½æ•°ã€‚

    Args:
        dataset: HuggingFace Dataset å¯¹è±¡
        tokenizer: åˆ†è¯å™¨
        batch_size: æ‰¹æ¬¡å¤§å°
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°ï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨ï¼‰
        collate_fn: è‡ªå®šä¹‰ collate å‡½æ•°ï¼ˆNone è¡¨ç¤ºä½¿ç”¨é»˜è®¤ï¼‰

    Returns:
        é…ç½®å¥½çš„ DataLoader
    """
    # è‡ªåŠ¨ç¡®å®š worker æ•°é‡
    if num_workers is None:
        num_workers = min(8, max(0, (os.cpu_count() or 1) - 1))

    # é»˜è®¤ collate å‡½æ•°
    if collate_fn is None:
        def default_collate(batch):
            # æå–å­—æ®µ
            input_ids = [item["input_ids"] for item in batch]
            attention_mask = [item["attention_mask"] for item in batch]
            labels = [item["label"] for item in batch]

            # å¡«å……
            padded = tokenizer.pad(
                {"input_ids": input_ids, "attention_mask": attention_mask},
                padding=True,
                max_length=max_length,
                return_tensors="pt",
            )

            return {
                "input_ids": padded["input_ids"],
                "attention_mask": padded["attention_mask"],
                "labels": torch.tensor(labels, dtype=torch.long),
            }

        collate_fn = default_collate

    # åˆ›å»º DataLoader
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=torch.cuda.is_available(),  # è‡ªåŠ¨ä¼˜åŒ–
        persistent_workers=(num_workers > 0),  # ä¿æŒ worker è¿›ç¨‹
    )

    return loader

def apply_noise_augmentation(
    dataset,
    noise_level: float,
    tokenizer: PreTrainedTokenizer,
    text_column: str = "text",
):
    """ä¸ºæ•°æ®é›†åº”ç”¨å™ªå£°å¢å¼ºã€‚

    Args:
        dataset: åŸå§‹æ•°æ®é›†
        noise_level: å™ªå£°æ¯”ä¾‹ (0.0 - 1.0)
        tokenizer: åˆ†è¯å™¨
        text_column: æ–‡æœ¬å­—æ®µåç§°

    Returns:
        å¢å¼ºåçš„æ•°æ®é›†
    """
    if noise_level == 0.0:
        return dataset  # æ— å™ªå£°ï¼Œç›´æ¥è¿”å›

    def add_noise(example):
        # ç®€åŒ–çš„å™ªå£°æ³¨å…¥é€»è¾‘
        tokens = example[text_column].split()
        # ... å™ªå£°é€»è¾‘
        example[text_column] = " ".join(tokens)
        return example

    return dataset.map(add_noise)
```

**ä½¿ç”¨é‡æ„åçš„å·¥å…·**:

```python
# ============================================================
# æ–‡ä»¶: fif_mvp/data/sst2.py (é‡æ„å)
# ============================================================

from fif_mvp.data.common import create_dataloader, apply_noise_augmentation

def get_sst2_loaders(
    tokenizer,
    batch_size: int = 32,
    max_length: int = 128,
    noise_level: float = 0.0,
    num_workers: Optional[int] = None,
):
    """åŠ è½½ SST-2 æ•°æ®é›†ï¼ˆé‡æ„åï¼‰ã€‚"""
    from datasets import load_dataset

    # åŠ è½½æ•°æ®
    dataset = load_dataset("glue", "sst2")
    train_data = dataset["train"]
    val_data = dataset["validation"]

    # åº”ç”¨å™ªå£°ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if noise_level > 0:
        train_data = apply_noise_augmentation(
            train_data, noise_level, tokenizer, text_column="sentence"
        )

    # Tokenization
    def tokenize(example):
        return tokenizer(
            example["sentence"],
            truncation=True,
            max_length=max_length,
        )

    train_data = train_data.map(tokenize, batched=True)
    val_data = val_data.map(tokenize, batched=True)

    # åˆ›å»º DataLoaderï¼ˆä½¿ç”¨å…±äº«å‡½æ•°ï¼‰
    train_loader = create_dataloader(
        train_data,
        tokenizer,
        batch_size,
        max_length,
        shuffle=True,
        num_workers=num_workers,
    )

    val_loader = create_dataloader(
        val_data,
        tokenizer,
        batch_size,
        max_length,
        shuffle=False,
        num_workers=num_workers,
    )

    return train_loader, val_loader
```

**æ•ˆæœ**:
- æ¶ˆé™¤ 26 è¡Œé‡å¤ä»£ç 
- Bug ä¿®å¤åªéœ€ä¸€å¤„ä¿®æ”¹
- æ–°æ•°æ®é›†å¤ç”¨ç°æœ‰é€»è¾‘

---

## è§£å†³æ–¹æ¡ˆ 11-24: å·¥ç¨‹è´¨é‡æå‡ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆä»¥æ¸…å•å½¢å¼åˆ—å‡ºå…³é”®è¦ç‚¹ï¼š

### è§£å†³æ–¹æ¡ˆ 11: æ·»åŠ æµ‹è¯•åŸºç¡€è®¾æ–½

```bash
# ç›®å½•ç»“æ„
tests/
â”œâ”€â”€ conftest.py           # pytest é…ç½®å’Œå…±äº« fixtures
â”œâ”€â”€ fixtures/             # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ reference_outputs.pt
â”‚   â””â”€â”€ sample_data.json
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_friction_layer.py
â”‚   â”œâ”€â”€ test_sparse_utils.py
â”‚   â””â”€â”€ test_metrics.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_training_loop.py
â”‚   â””â”€â”€ test_data_pipeline.py
â””â”€â”€ benchmark/
    â”œâ”€â”€ benchmark_forward_pass.py
    â””â”€â”€ benchmark_data_loading.py

# pytest é…ç½®
# pytest.ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --strict-markers
    --tb=short
    --cov=fif_mvp
    --cov-report=html
    --cov-report=term-missing:skip-covered
```

### è§£å†³æ–¹æ¡ˆ 12: æ·»åŠ  CI/CD

```yaml
# .github/workflows/ci.yml
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11']

    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          pip install -r requirements-dev.txt

      - name: Run tests
        run: pytest tests/ --cov --cov-report=xml

      - name: Type check
        run: mypy fif_mvp --strict

      - name: Code style
        run: |
          black --check fif_mvp
          isort --check fif_mvp
```

### è§£å†³æ–¹æ¡ˆ 13-24: å…¶ä»–æ”¹è¿›

| ID | è§£å†³æ–¹æ¡ˆ | å…³é”®å®æ–½æ­¥éª¤ | é¢„è®¡æ—¶é—´ |
|----|----------|-------------|---------|
| 13 | æ·»åŠ ç±»å‹æ³¨è§£ | ä½¿ç”¨ mypy --strict, é€æ–‡ä»¶ä¿®å¤ | 3 å¤© |
| 14 | ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ | æ›¿æ¢ print ä¸º logger, é…ç½®æ—¥å¿—æ ¼å¼ | 1 å¤© |
| 15 | æå– magic numbers | åˆ›å»º constants.py, é›†ä¸­ç®¡ç† | 0.5 å¤© |
| 16 | æ·»åŠ æ–‡æ¡£æ³¨é‡Š | ä¸ºå¤æ‚å‡½æ•°æ·»åŠ  docstring | 2 å¤© |
| 17 | åˆ›å»º setup.py | æ”¯æŒ pip install -e . | 0.5 å¤© |
| 18 | æ·»åŠ  pre-commit | é…ç½® black, isort, flake8 | 0.5 å¤© |
| 19 | ç»Ÿä¸€å˜é‡å‘½å | Rename refactoring, ç¡®ä¿æµ‹è¯•é€šè¿‡ | 1 å¤© |
| 20-24 | å…¶ä»–å·¥ç¨‹æ”¹è¿› | ... | ... |

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æ–½è·¯çº¿å›¾

## æ€»ä½“æ—¶é—´çº¿ï¼ˆ6 å‘¨è®¡åˆ’ï¼‰

### Week 1: ç´§æ€¥ä¿®å¤ (P0)
- âœ… Day 1-2: ä¿®å¤ç¼“å­˜å†…å­˜æ³„æ¼ (è§£å†³æ–¹æ¡ˆ 1)
- âœ… Day 3-4: å›ºå®šä¾èµ–ç‰ˆæœ¬ (è§£å†³æ–¹æ¡ˆ 4)
- âœ… Day 5: æ”¹è¿›å¼‚å¸¸å¤„ç† (è§£å†³æ–¹æ¡ˆ 5)

### Week 2: æ€§èƒ½ä¼˜åŒ– (P0)
- âœ… Day 1-2: ä¼˜åŒ– GPUâ†”CPU ä¼ è¾“ (è§£å†³æ–¹æ¡ˆ 2)
- âœ… Day 3: å‘é‡åŒ–æ··æ·†çŸ©é˜µ (è§£å†³æ–¹æ¡ˆ 3)
- âœ… Day 4-5: æ€§èƒ½åŸºå‡†æµ‹è¯•å’ŒéªŒè¯

### Week 3: æµ‹è¯•åŸºç¡€è®¾æ–½ (P1)
- âœ… Day 1-2: æ­å»º pytest æ¡†æ¶ (è§£å†³æ–¹æ¡ˆ 11)
- âœ… Day 3-4: ç¼–å†™æ ¸å¿ƒæ¨¡å—å•å…ƒæµ‹è¯•
- âœ… Day 5: é›†æˆæµ‹è¯•å’Œ CI é…ç½® (è§£å†³æ–¹æ¡ˆ 12)

### Week 4: ä»£ç é‡æ„ (P1)
- âœ… Day 1-3: æ‹†åˆ†è¶…é•¿å‡½æ•° (è§£å†³æ–¹æ¡ˆ 6)
- âœ… Day 4-5: æå–å…±äº«é€»è¾‘ (è§£å†³æ–¹æ¡ˆ 7)

### Week 5: å·¥ç¨‹è´¨é‡ (P2)
- âœ… Day 1-2: æ·»åŠ ç±»å‹æ³¨è§£
- âœ… Day 3: ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
- âœ… Day 4-5: æ–‡æ¡£å’Œæ³¨é‡Š

### Week 6: æ”¶å°¾å’Œå‘å¸ƒ
- âœ… Day 1-2: ä»£ç å®¡æŸ¥å’Œä¿®å¤
- âœ… Day 3-4: å®Œæ•´å›å½’æµ‹è¯•
- âœ… Day 5: å‘å¸ƒ v1.1.0

---

## é£é™©ç®¡ç†

### é«˜é£é™©ä»»åŠ¡

| ä»»åŠ¡ | é£é™© | ç¼“è§£æªæ–½ | å›æ»šè®¡åˆ’ |
|------|------|---------|---------|
| GPU ä¼˜åŒ– (æ–¹æ¡ˆ 2) | å¯èƒ½æ”¹å˜æ•°å€¼ç»“æœ | ä¸¥æ ¼çš„æ•°å€¼æµ‹è¯• (1e-6 å®¹å·®) | ä¿ç•™åŸå®ç°ä½œä¸º fallback |
| å‡½æ•°é‡æ„ (æ–¹æ¡ˆ 6) | å¼•å…¥æ–° bug | æ¯æ¬¡é‡æ„åè¿è¡Œå®Œæ•´æµ‹è¯• | Git revert |
| ä¾èµ–æ›´æ–° (æ–¹æ¡ˆ 4) | ç ´åå…¼å®¹æ€§ | é”å®šå½“å‰ç¨³å®šç‰ˆæœ¬ | requirements-legacy.txt |

### æ•°å€¼ç¨³å®šæ€§ä¿è¯

**å…³é”®åŸåˆ™**: æ‰€æœ‰ä¼˜åŒ–å¿…é¡»é€šè¿‡æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•

```python
# tests/test_numerical_stability.py
import torch

def test_end_to_end_numerical_consistency():
    """ç«¯åˆ°ç«¯æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰"""

    # åŠ è½½åŸºçº¿ç»“æœï¼ˆä¼˜åŒ–å‰è¿è¡Œå¹¶ä¿å­˜ï¼‰
    baseline = torch.load("tests/fixtures/baseline_training_epoch1.pt")

    # è¿è¡Œä¼˜åŒ–åçš„ä»£ç 
    torch.manual_seed(42)  # å›ºå®šç§å­
    from fif_mvp.train import TrainLoop
    # ... è¿è¡Œä¸€ä¸ª epoch

    # é€é¡¹å¯¹æ¯”
    torch.testing.assert_close(
        current_loss, baseline["loss"],
        rtol=1e-5, atol=1e-6,
        msg="è®­ç»ƒæŸå¤±ä¸ä¸€è‡´"
    )
    torch.testing.assert_close(
        current_accuracy, baseline["accuracy"],
        rtol=1e-5, atol=1e-6,
        msg="å‡†ç¡®ç‡ä¸ä¸€è‡´"
    )

    # å¦‚æœæµ‹è¯•å¤±è´¥ â†’ æ‹’ç»åˆå¹¶ PR
```

---

## åº¦é‡æŒ‡æ ‡

### æ”¹è¿›å‰åå¯¹æ¯”

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å | æå‡ |
|------|--------|--------|------|
| æµ‹è¯•è¦†ç›–ç‡ | 0% | 80%+ | âˆ |
| ä»£ç é‡å¤ç‡ | 15% | <5% | 67% â†“ |
| å¹³å‡å‡½æ•°é•¿åº¦ | 45 è¡Œ | 25 è¡Œ | 44% â†“ |
| è®­ç»ƒé€Ÿåº¦ (SST-2) | 100% | 110-120% | 10-20% â†‘ |
| å†…å­˜å³°å€¼ | ä¸ç¨³å®š | ç¨³å®š | é£é™©æ¶ˆé™¤ |
| ä¾èµ–å¯å¤ç°æ€§ | å¦ | æ˜¯ | âœ… |
| é™é»˜å¤±è´¥æ•°é‡ | 3 | 0 | 100% â†“ |

### æŒç»­ç›‘æ§

```yaml
# .github/workflows/metrics.yml
# æ¯å‘¨è‡ªåŠ¨ç”Ÿæˆä»£ç è´¨é‡æŠ¥å‘Š

name: Code Quality Metrics

on:
  schedule:
    - cron: '0 0 * * 0'  # æ¯å‘¨æ—¥è¿è¡Œ

jobs:
  metrics:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Run coverage
        run: pytest --cov --cov-report=json

      - name: Check code complexity
        run: radon cc fif_mvp -a -j > complexity.json

      - name: Check code duplication
        run: pylint --duplicate-code fif_mvp > duplication.txt

      - name: Upload report
        uses: actions/upload-artifact@v3
        with:
          name: quality-report
          path: |
            coverage.json
            complexity.json
            duplication.txt
```

---

# ç¬¬å››éƒ¨åˆ†ï¼šæ‰§è¡Œæ£€æŸ¥æ¸…å•

## æ¯ä¸ªè§£å†³æ–¹æ¡ˆçš„éªŒæ”¶æ ‡å‡†

### è§£å†³æ–¹æ¡ˆ 1: ç¼“å­˜ä¿®å¤

- [ ] `_build_window_edges_cpu` ä½¿ç”¨ `@lru_cache(maxsize=128)`
- [ ] è®¾å¤‡ç¼“å­˜é™åˆ¶ä¸º 256 é¡¹
- [ ] é€šè¿‡æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•ï¼ˆ`test_cache_fix_numerical_equivalence`ï¼‰
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 95%
- [ ] å†…å­˜ä½¿ç”¨ç¨³å®šï¼ˆ1000 æ¬¡è°ƒç”¨åä¸å¢é•¿ï¼‰

### è§£å†³æ–¹æ¡ˆ 2: GPU ä¼˜åŒ–

- [ ] æ¶ˆé™¤ `lengths.tolist()` è°ƒç”¨
- [ ] ä½¿ç”¨ `torch.unique` è¿›è¡Œåˆ†æ¡¶
- [ ] é€šè¿‡ç«¯åˆ°ç«¯æ•°å€¼æµ‹è¯•ï¼ˆè¯¯å·® < 1e-6ï¼‰
- [ ] å‰å‘ä¼ æ’­åŠ é€Ÿ > 5%
- [ ] æ¢¯åº¦æ•°å€¼ç¨³å®šæ€§éªŒè¯é€šè¿‡

### è§£å†³æ–¹æ¡ˆ 3: å‘é‡åŒ–

- [ ] ä½¿ç”¨ `np.bincount` æ›¿ä»£ for å¾ªç¯
- [ ] æ·»åŠ è¾“å…¥éªŒè¯ï¼ˆè¾¹ç•Œæ£€æŸ¥ï¼‰
- [ ] é€šè¿‡å¤§è§„æ¨¡æ•°æ®æµ‹è¯•ï¼ˆ10,000 æ ·æœ¬ï¼‰
- [ ] æ€§èƒ½æå‡ > 10x

### è§£å†³æ–¹æ¡ˆ 4: ä¾èµ–é”å®š

- [ ] `requirements.txt` æ‰€æœ‰ç‰ˆæœ¬ä½¿ç”¨ `==`
- [ ] åˆ›å»º `requirements-dev.txt`
- [ ] åˆ›å»º `requirements-lock.txt`ï¼ˆpip freezeï¼‰
- [ ] CI éªŒè¯å¤šç¯å¢ƒå®‰è£…æˆåŠŸ
- [ ] æ–‡æ¡£æ›´æ–°ï¼ˆREADME.mdï¼‰

### è§£å†³æ–¹æ¡ˆ 5: å¼‚å¸¸å¤„ç†

- [ ] ç§»é™¤æ‰€æœ‰ `except Exception: pass`
- [ ] æ‰€æœ‰å¼‚å¸¸æ•è·éƒ½è®°å½•æ—¥å¿—
- [ ] ä½¿ç”¨å…·ä½“å¼‚å¸¸ç±»å‹ï¼ˆé¿å…è£¸ Exceptionï¼‰
- [ ] åˆ›å»º `utils/error_handling.py`
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰å¼‚å¸¸è·¯å¾„

### è§£å†³æ–¹æ¡ˆ 6-10: é‡æ„

- [ ] `_run_cli()` é•¿åº¦ < 60 è¡Œ
- [ ] æ¯ä¸ªå­å‡½æ•°èŒè´£å•ä¸€
- [ ] æå–çš„å…±äº«å‡½æ•°æœ‰æ–‡æ¡£å’Œæµ‹è¯•
- [ ] ä»£ç é‡å¤ç‡ < 5%
- [ ] æ‰€æœ‰é‡æ„é€šè¿‡å›å½’æµ‹è¯•

### è§£å†³æ–¹æ¡ˆ 11-12: æµ‹è¯•å’Œ CI

- [ ] pytest é…ç½®å®Œæ•´ï¼ˆpytest.iniï¼‰
- [ ] æ ¸å¿ƒæ¨¡å—æµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] GitHub Actions é…ç½®æ­£ç¡®
- [ ] CI åœ¨ PR æ—¶è‡ªåŠ¨è¿è¡Œ
- [ ] æµ‹è¯•æ–‡æ¡£ï¼ˆtests/README.mdï¼‰

---

## æ¯æ—¥ Standup æ£€æŸ¥æ¸…å•

### å¼€å‘è€…æ¯æ—¥è‡ªæ£€

```markdown
## ä»Šæ—¥å·¥ä½œ
- [ ] ä»»åŠ¡: _______________
- [ ] åˆ†æ”¯: _______________
- [ ] çŠ¶æ€: _______________

## è´¨é‡æ£€æŸ¥
- [ ] æ‰€æœ‰æ–°ä»£ç æœ‰ç±»å‹æ³¨è§£
- [ ] æ·»åŠ äº†å•å…ƒæµ‹è¯•ï¼ˆå¦‚é€‚ç”¨ï¼‰
- [ ] é€šè¿‡ `make lint`ï¼ˆblack, isort, mypyï¼‰
- [ ] é€šè¿‡ `make test`ï¼ˆæ‰€æœ‰æµ‹è¯•ï¼‰
- [ ] æ›´æ–°äº†æ–‡æ¡£ï¼ˆå¦‚é€‚ç”¨ï¼‰

## æ•°å€¼éªŒè¯
- [ ] å¦‚ä¿®æ”¹æ ¸å¿ƒé€»è¾‘ï¼Œå·²è¿è¡Œæ•°å€¼å¯¹æ¯”æµ‹è¯•
- [ ] æ— æµ®ç‚¹ç²¾åº¦é€€åŒ–ï¼ˆè¯¯å·® < 1e-6ï¼‰
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼ˆæ— é€€åŒ–ï¼‰

## æäº¤å‰
- [ ] Commit æ¶ˆæ¯æ¸…æ™°ï¼ˆéµå¾ª Conventional Commitsï¼‰
- [ ] ä»£ç å®¡æŸ¥ self-review
- [ ] æ— è°ƒè¯•ä»£ç ï¼ˆprint, pdb, TODOï¼‰
```

---

## ç‰ˆæœ¬å‘å¸ƒæ£€æŸ¥æ¸…å•

### v1.1.0 å‘å¸ƒå‰

```markdown
## åŠŸèƒ½å®Œæ•´æ€§
- [ ] æ‰€æœ‰ P0 é—®é¢˜å·²ä¿®å¤
- [ ] æ‰€æœ‰ P1 é—®é¢˜å·²ä¿®å¤ï¼ˆæˆ–æ¨è¿Ÿåˆ°ä¸‹ä¸€ç‰ˆæœ¬ï¼‰
- [ ] å˜æ›´æ—¥å¿—å·²æ›´æ–°ï¼ˆCHANGELOG.mdï¼‰

## æµ‹è¯•éªŒè¯
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•æ— é€€åŒ–
- [ ] ç«¯åˆ°ç«¯è®­ç»ƒéªŒè¯ï¼ˆSST-2 + SNLIï¼‰
- [ ] æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡

## æ–‡æ¡£æ›´æ–°
- [ ] README.md æ›´æ–°ï¼ˆå®‰è£…è¯´æ˜ã€ä½¿ç”¨ç¤ºä¾‹ï¼‰
- [ ] API æ–‡æ¡£ç”Ÿæˆï¼ˆSphinxï¼‰
- [ ] è¿ç§»æŒ‡å—ï¼ˆå¦‚æœ‰ç ´åæ€§å˜æ›´ï¼‰
- [ ] å˜æ›´æ—¥å¿—è¯¦ç»†è¯´æ˜

## å‘å¸ƒæµç¨‹
- [ ] åˆå¹¶æ‰€æœ‰ PR åˆ° main
- [ ] æ›´æ–°ç‰ˆæœ¬å·ï¼ˆfif_mvp/__init__.pyï¼‰
- [ ] åˆ›å»º Git tag: v1.1.0
- [ ] æ¨é€ tag è§¦å‘ CI å‘å¸ƒ
- [ ] GitHub Release å‘å¸ƒè¯´æ˜
- [ ] é€šçŸ¥ç”¨æˆ·ï¼ˆå¦‚æœ‰é‚®ä»¶åˆ—è¡¨ï¼‰
```

---

# é™„å½• A: å‚è€ƒä»£ç ç‰‡æ®µ

## A.1 æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•æ¡†æ¶

```python
# tests/utils/numerical_test.py
import torch
from pathlib import Path

class NumericalConsistencyTester:
    """æ•°å€¼ä¸€è‡´æ€§æµ‹è¯•å·¥å…·"""

    def __init__(self, baseline_dir: str = "tests/fixtures/baseline"):
        self.baseline_dir = Path(baseline_dir)
        self.baseline_dir.mkdir(parents=True, exist_ok=True)

    def save_baseline(self, name: str, data: dict):
        """ä¿å­˜åŸºçº¿æ•°æ®ï¼ˆç”±åŸä»£ç ç”Ÿæˆï¼‰"""
        torch.save(data, self.baseline_dir / f"{name}.pt")

    def compare(self, name: str, current: dict, rtol=1e-5, atol=1e-6):
        """å¯¹æ¯”å½“å‰ç»“æœä¸åŸºçº¿"""
        baseline = torch.load(self.baseline_dir / f"{name}.pt")

        for key in baseline:
            if key not in current:
                raise AssertionError(f"Missing key in current: {key}")

            torch.testing.assert_close(
                current[key], baseline[key],
                rtol=rtol, atol=atol,
                msg=f"Mismatch in {name}.{key}"
            )

    def benchmark(self, name: str, func, *args, **kwargs):
        """è¿è¡Œå¹¶å¯¹æ¯”æ€§èƒ½"""
        import time

        # é¢„çƒ­
        for _ in range(10):
            func(*args, **kwargs)

        # åŸºå‡†æµ‹è¯•
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start = time.perf_counter()

        for _ in range(100):
            result = func(*args, **kwargs)

        torch.cuda.synchronize() if torch.cuda.is_available() else None
        elapsed = time.perf_counter() - start

        return result, elapsed / 100

# ä½¿ç”¨ç¤ºä¾‹
tester = NumericalConsistencyTester()

# 1. ä¿å­˜åŸºçº¿ï¼ˆä»…åœ¨ä¿®æ”¹å‰è¿è¡Œä¸€æ¬¡ï¼‰
# baseline_result, _ = tester.benchmark("friction_forward", layer, hidden, mask)
# tester.save_baseline("friction_forward", baseline_result)

# 2. å¯¹æ¯”ä¼˜åŒ–åçš„ç»“æœ
current_result, time_new = tester.benchmark("friction_forward", layer, hidden, mask)
tester.compare("friction_forward", current_result)
print(f"âœ“ æ•°å€¼ä¸€è‡´æ€§éªŒè¯é€šè¿‡ï¼Œè€—æ—¶: {time_new*1000:.2f}ms")
```

## A.2 è‡ªåŠ¨åŒ–é‡æ„è„šæœ¬

```python
# scripts/refactor_helpers.py
import ast
import re
from pathlib import Path

def extract_function(
    file_path: str,
    function_name: str,
    start_line: int,
    end_line: int,
    new_function_name: str
):
    """è‡ªåŠ¨æå–ä»£ç å—ä¸ºç‹¬ç«‹å‡½æ•°"""

    with open(file_path, 'r') as f:
        lines = f.readlines()

    # æå–ç›®æ ‡ä»£ç å—
    extracted = lines[start_line-1:end_line]

    # åˆ†æå˜é‡ä¾èµ–ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
    # å®é™…åº”ä½¿ç”¨ AST åˆ†æ

    # ç”Ÿæˆæ–°å‡½æ•°
    new_func = f"""
def {new_function_name}(...):
    \"\"\"TODO: Add docstring\"\"\"
{''.join(extracted)}
    return result
"""

    # åœ¨åŸä½ç½®æ›¿æ¢ä¸ºå‡½æ•°è°ƒç”¨
    lines[start_line-1:end_line] = [f"    result = {new_function_name}(...)\n"]

    # å†™å›æ–‡ä»¶
    with open(file_path, 'w') as f:
        f.writelines(lines)

    print(f"âœ“ æå– {new_function_name} åˆ° {file_path}")
    return new_func

# ä½¿ç”¨ç¤ºä¾‹
# extract_function(
#     "fif_mvp/cli/run_experiment.py",
#     "_run_cli",
#     330, 345,
#     "_setup_directories"
# )
```

---

# é™„å½• B: å¿«é€Ÿå‚è€ƒ

## B.1 å¸¸ç”¨å‘½ä»¤

```bash
# å¼€å‘ç¯å¢ƒè®¾ç½®
python -m venv venv
source venv/bin/activate
pip install -r requirements-dev.txt
pre-commit install

# è¿è¡Œæµ‹è¯•
pytest tests/                      # æ‰€æœ‰æµ‹è¯•
pytest tests/unit/                 # ä»…å•å…ƒæµ‹è¯•
pytest -k test_friction_layer      # ç‰¹å®šæµ‹è¯•
pytest --cov --cov-report=html     # è¦†ç›–ç‡æŠ¥å‘Š

# ä»£ç è´¨é‡æ£€æŸ¥
black fif_mvp tests                # æ ¼å¼åŒ–
isort fif_mvp tests                # å¯¼å…¥æ’åº
mypy fif_mvp --strict              # ç±»å‹æ£€æŸ¥
flake8 fif_mvp                     # Lint æ£€æŸ¥

# æ€§èƒ½åˆ†æ
python -m line_profiler script.py  # è¡Œçº§æ€§èƒ½åˆ†æ
python -m memory_profiler script.py # å†…å­˜åˆ†æ

# ç”ŸæˆåŸºçº¿æ•°æ®
python scripts/generate_baseline.py --dataset sst2 --output tests/fixtures/

# æ•°å€¼éªŒè¯
python tests/validate_numerical_consistency.py
```

## B.2 æ–‡ä»¶æ”¹åŠ¨æ¸…å•

| æ–‡ä»¶è·¯å¾„ | æ”¹åŠ¨ç±»å‹ | è¡Œæ•°å˜åŒ– | é£é™© |
|---------|---------|---------|------|
| `utils/sparse.py` | é‡æ„ | +30, -20 | ä¸­ |
| `models/friction_layer.py` | ä¼˜åŒ– | +15, -5 | é«˜ |
| `train/metrics.py` | ä¼˜åŒ– | +10, -8 | ä½ |
| `requirements.txt` | ä¿®æ”¹ | +0, -0 | ä½ |
| `cli/run_experiment.py` | é‡æ„ | +80, -120 | ä¸­ |
| `data/common.py` | æ–°å¢ | +150, -0 | ä½ |
| `utils/error_handling.py` | æ–°å¢ | +100, -0 | ä½ |
| `tests/` | æ–°å¢ | +2000, -0 | - |

---

# æ€»ç»“

æœ¬æ–¹æ¡ˆæä¾›äº† **24 é¡¹æŠ€æœ¯å€ºåŠ¡çš„å®Œæ•´è§£å†³è·¯å¾„**ï¼Œéµå¾ªä»¥ä¸‹æ ¸å¿ƒåŸåˆ™ï¼š

1. **é›¶ç ´åæ€§**: æ‰€æœ‰æ”¹è¿›ä¿è¯æ•°å€¼ç»“æœä¸å˜
2. **å¯éªŒè¯æ€§**: æ¯ä¸ªæ”¹è¿›éƒ½æœ‰è‡ªåŠ¨åŒ–æµ‹è¯•
3. **å¯å›æ»šæ€§**: æ¯ä¸ªå˜æ›´éƒ½å¯ä»¥å®‰å…¨æ’¤é”€
4. **æ¸è¿›å¼**: æŒ‰ä¼˜å…ˆçº§åˆ†é˜¶æ®µå®æ–½
5. **æ–‡æ¡£åŒ–**: æ‰€æœ‰å˜æ›´éƒ½æœ‰è¯¦ç»†è¯´æ˜

**ç«‹å³å¯æ‰§è¡Œçš„é¦–è¦ä»»åŠ¡**:
1. Week 1: ä¿®å¤ç¼“å­˜å†…å­˜æ³„æ¼ + å›ºå®šä¾èµ–ç‰ˆæœ¬
2. Week 2: GPU ä¼˜åŒ– + å‘é‡åŒ–è®¡ç®—
3. Week 3: æ­å»ºæµ‹è¯•åŸºç¡€è®¾æ–½

**é¢„æœŸæˆæœ**:
- ä»£ç è´¨é‡ä»"ç²—ç³™"æå‡åˆ°"è‰¯å¥½"
- æµ‹è¯•è¦†ç›–ç‡ä» 0% æå‡åˆ° 80%+
- è®­ç»ƒæ€§èƒ½æå‡ 10-20%
- æ¶ˆé™¤æ‰€æœ‰å·²çŸ¥çš„å†…å­˜æ³„æ¼å’Œé™é»˜å¤±è´¥
- å»ºç«‹æŒç»­é›†æˆå’Œè´¨é‡ç›‘æ§ä½“ç³»

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–¹æ¡ˆåº”æ¯å‘¨å®¡æŸ¥ä¸€æ¬¡ï¼Œæ ¹æ®å®é™…è¿›å±•æ›´æ–°çŠ¶æ€ã€‚
