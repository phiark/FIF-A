# FIF-A: Frictional Interaction Field for Robust NLP

**é¡¹ç›®çŠ¶æ€**
- å½“å‰ç‰ˆæœ¬ï¼šv1.2.0 (ğŸ”„ è¿›è¡Œä¸­)
- æœ€æ–°ç¨³å®šç‰ˆï¼šv1.0.4 (2024-12-02)
- è®ºæ–‡çŠ¶æ€ï¼šå®éªŒé˜¶æ®µ
- æ–‡æ¡£ï¼š[PROJECT_TRACKER](PROJECT_TRACKER.md) | [WORK_BOARD](WORK_BOARD.md) | [PHASE_RESULTS](PHASE_RESULTS.md) | [docs/](docs/)

---

## æ¦‚è¿°

æœ¬é¡¹ç›®ç ”ç©¶åœ¨è½»é‡çº§ Transformer ä¸­å¼•å…¥**æ‘©æ“¦äº¤äº’åœº (Frictional Interaction Field, FIF)** å¯¹å™ªå£°é²æ£’æ€§çš„å½±å“ï¼Œå¹¶æ¢ç´¢èƒ½é‡ä¿¡å·ä½œä¸ºç½®ä¿¡åº¦ä»£ç†çš„å¯è¡Œæ€§ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼š
- **åŠ¨æ€æ‘©æ“¦å±‚**ï¼šé€šè¿‡è¿­ä»£ä¼˜åŒ–éšçŠ¶æ€ï¼Œå¼•å…¥å¯å­¦ä¹ çš„æ‘©æ“¦ç³»æ•° Î¼
- **èƒ½é‡æ­£åˆ™åŒ–**ï¼šä½¿ç”¨èƒ½é‡ä¿¡å· (`E = 0.5 Î£ Î¼_ij ||h_i - h_j||^2`) ç›‘ç£æ¨¡å‹ç½®ä¿¡åº¦
- **å™ªå£°æ¡ä»¶åŒ–**ï¼šè®­ç»ƒæ—¶æ··åˆå¤šå¼ºåº¦å™ªå£°æ•°æ®

**ç ”ç©¶é—®é¢˜**ï¼š
1. FIF å±‚èƒ½å¦æå‡æ¨¡å‹åœ¨å™ªå£°æ•°æ®ä¸Šçš„é²æ£’æ€§ï¼Ÿ
2. èƒ½é‡ä¿¡å·æ˜¯å¦èƒ½æœ‰æ•ˆé¢„æµ‹é¢„æµ‹é”™è¯¯ï¼Ÿ
3. æœ€ä½³èƒ½é‡æ­£åˆ™åŒ–ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ

**å½“å‰æœ€ä½³ç»“æœ** (v1.0.4):
- SST-2 Low Noisy: Hybrid **0.808** acc (vs Baseline 0.782)
- ECEé™ä½: 0.124 â†’ **0.064**
- âš ï¸ SNLIä»»åŠ¡å¾…æ”¹è¿› (Hybrid 0.69 vs Baseline 0.76)

---

## å®éªŒå†å²

This experimental repo benchmarks whether introducing a Frictional Interaction Field (FIF, ä¿¡æ¯æ‘©æ“¦å±‚) into a lightweight Transformer improves robustness and whether the induced energy signal correlates with prediction errors. Since v1.0.0 we:

- inject `clean + low/med/high` noise directly into the SST-2 training split,
- condition encoders on `noise_level` embeddings,
- upgrade the friction layer with dynamic Î¼ã€åº¦å½’ä¸€åŒ–åŠ Î· è¡°å‡ + å¹³æ»‘ï¼Œ
- add log-energy regularization/metrics to tighten â€œèƒ½é‡â‰ˆé”™è¯¯æ¦‚ç‡â€çš„å‡è®¾ã€‚

## Setup

```bash
python3 -m venv .venv
. .venv/bin/activate
pip install -r requirements.txt
```

## Running experiments

### Quick Start (æ¨è)

```bash
# è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆSNLI + SST-2, baseline + hybridï¼‰
./run.sh

# å¿«é€Ÿé€‰æ‹©å™¨ - è¿è¡Œç‰¹å®šå®éªŒ
./quick.sh snli          # SNLI baseline + hybrid
./quick.sh sst2          # SST-2 baseline + hybrid
./quick.sh baseline      # ä¸¤ä¸ª baseline å®éªŒ
./quick.sh hybrid        # ä¸¤ä¸ª hybrid å®éªŒ

# è¿è¡Œå•ä¸ªå®éªŒ
./quick.sh snli_baseline
./quick.sh snli_hybrid
./quick.sh sst2_baseline
./quick.sh sst2_hybrid

# é¢„è§ˆå‘½ä»¤
./quick.sh snli --dry-run
```

æŸ¥çœ‹å®Œæ•´æŒ‡å—ï¼š[QUICK_START.md](QUICK_START.md)

### å®éªŒé…ç½®

æ‰€æœ‰å®éªŒé…ç½®åœ¨ `scripts/experiments.yaml`ï¼ˆ4ä¸ªæ ‡å‡†å®éªŒï¼‰ï¼š
- `snli_baseline` - SNLI Transformer baseline
- `snli_hybrid` - SNLI with FIF (K=3, rank regularization)
- `sst2_baseline` - SST-2 Transformer baseline
- `sst2_hybrid` - SST-2 with FIF (K=3, rank regularization)

æ—§å®éªŒï¼ˆnoisy variants, v1.1.0 failuresï¼‰å­˜æ¡£åœ¨ `scripts/experiments_archive.yaml`ã€‚

ç»“æœä¿å­˜åœ¨ `./result/` ç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰ã€‚The runner auto-detects the best accelerator in priority order (CUDA/NVIDIA or AMD ROCm, Apple MPS, then CPU) and only enables DDP when multiple CUDA devices are available.

### Multi-backend / Multi-GPU

- CUDA (NVIDIA/AMD ROCm): DDP is enabled automatically by the runner when `torch.cuda.device_count()>1`. TF32 tuning is skipped on AMD to avoid unsupported settings.
- Apple MPS: runs single-process; DDP is skipped.
- CPU: runs single-process.
- You can still call `python -m fif_mvp.cli.run_experiment ...` manually; pass `--ddp` for single-node multi-GPU CUDA launches or `torchrun ...` if you prefer manual control. DataParallel remains available but DDP is recommended to avoid gather warnings.

Key CLI knobs:

- `--train_noise_levels clean,low,med,high` controls which noise intensities are duplicated in the training split (default mixeså…¨éƒ¨å››æ¡£)ï¼›
- `--energy_reg_weight 1e-4` åœ¨è®­ç»ƒæŸå¤±ä¸­åŠ å…¥èƒ½é‡æ­£åˆ™ï¼›
- `--energy_reg_scope {all,last}` æ§åˆ¶èƒ½é‡æ­£åˆ™æ–½åŠ åœ¨å…¨éƒ¨æ‘©æ“¦å±‚èƒ½é‡ä¹‹å’Œè¿˜æ˜¯ä»…æœ€åä¸€å±‚ï¼ˆé»˜è®¤ `last`ï¼‰ï¼›
- `--energy_reg_target {absolute,normalized,margin,rank}`ï¼šé»˜è®¤ `rank`ï¼Œå¯¹ batch å½’ä¸€åŒ–èƒ½é‡æ‰§è¡Œæ’åº/é—´éš”çº¦æŸï¼›`absolute` ç›´æ¥æƒ©ç½š `log1p(E)`ï¼Œ`normalized` æƒ©ç½š batch å†… `log1p(E)` æ–¹å·®ï¼›`--energy_reg_mode` å·²åºŸå¼ƒï¼Œä»ä¿ç•™å‘åå…¼å®¹ã€‚
- `--energy_rank_margin`ã€`--energy_rank_topk`ï¼šç”¨äº `margin/rank` ç›®æ ‡æ—¶çš„é—´éš”ä¸å¯¹æ¯”çš„ hardest é”™è¯¯æ ·æœ¬æ•°é‡ã€‚
- `--energy_rank_fallback {absolute,none}`ï¼šå½“ä¸€ä¸ª batch å…¨å¯¹æˆ–å…¨é”™æ—¶çš„é€€åŒ–æ­£åˆ™ï¼ˆé»˜è®¤ `absolute` ç¡®ä¿ Î» ä»æœ‰æ¢¯åº¦ï¼‰ã€‚
- `--energy_eval_scope {auto,per_sample}`ï¼šmetrics/å‘Šè­¦ä½¿ç”¨å“ªç§èƒ½é‡ï¼Œ`auto` ä¸æ­£åˆ™ scope å¯¹é½ï¼ˆä¾‹å¦‚ä»…æœ«å±‚ï¼‰ï¼Œ`per_sample` ä½¿ç”¨è·¨å±‚æ±‚å’Œã€‚
- `--energy_metrics_source {normalized,raw}`ï¼šèƒ½é‡ç›¸å…³æ€§æŒ‡æ ‡é»˜è®¤ä½¿ç”¨ z-score èƒ½é‡ï¼Œå¯é€‰æ”¹å› rawã€‚
- `--energy_guard std_low=0.1,std_high=6,p90_high=8,factor=0.5,up=1.2,min_weight=1e-5,max=1e-3` åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§èƒ½é‡ä¸Šä¸‹ç•Œå¹¶è‡ªåŠ¨ä¸‹/ä¸Šè°ƒ Î»ï¼ˆå¯ç”¨ `--energy_guard off` ç¦ç”¨ï¼‰ï¼›
- `--energy_watch std=0.1,std_high=10,p90=0.5,p90_high=8,mean_low=0.1` åœ¨è®­ç»ƒ/éªŒè¯/æµ‹è¯•é˜¶æ®µè®°å½•èƒ½é‡å‘Šè­¦å¹¶å†™å…¥ `alerts.json`ï¼ˆå¯ç”¨ `--energy_watch off` å…³é—­ï¼‰ã€‚
- existing `--noise_intensity {low,med,high}` é€‰æ‹©éªŒè¯/æµ‹è¯•å™ªå£°å¼ºåº¦ã€‚
- Friction knobs: `--friction.eta_decay`, `--friction.mu_max`, `--friction.smooth_lambda`,
  `--friction.{normalize_laplacian,no_normalize_laplacian}`, `--friction.{recompute_mu,no_recompute_mu}`ã€‚
 - Data sampling: `--sortish_batches`ï¼ˆé DDPï¼‰ä¸ `--sortish_chunk_mult` æ§åˆ¶é•¿åº¦è¿‘ä¼¼åˆ†æ¡¶ï¼Œå‡å°‘ padding å¼€é”€ã€‚

## Result artifacts

Every run subdirectory contains:

* `config.json`, `env.txt`, `timing.json`
* `train_log.txt`, `metrics_epoch.csv`, `energy_epoch.csv`ï¼ˆè®°å½• `energy_log_mean/energy_std/p90`ã€`energy_norm_{mean,std,p90}`ã€`energy_alert` å’Œæ´»è·ƒçš„ Î»ï¼‰
* `test_summary.json` with `acc`, `macro_f1`, `loss`, `ece`, `energy_mean_test`, `energy_log_mean_test`, optional `energy_norm_*`, and energyâ€‘error metrics (`energy_auroc/auprc`, `coverage_aurc`, `coverage_risk_at_{80,90,95}`, energy åˆ†ä½)
* `confusion_matrix.csv`
* `energy_error_correlation.json`ï¼ˆåŒ…å« `pearson_r/auroc/auprc/aurc`ã€coverageâ€‘risk æ›²çº¿å­é‡‡æ ·ã€æ­£ç¡®/é”™è¯¯èƒ½é‡åˆ†ä½ï¼Œå¹¶è®°å½• energy_metrics_sourceï¼‰
* `alerts.json`ï¼ˆå½“ `--energy_watch` æˆ– `--energy_guard` è§¦å‘äº‹ä»¶æ—¶ç”Ÿæˆï¼Œåˆ—å‡ºåŸå› ä¸ Î» å›é€€ï¼‰
* (Noisy SST-2 only) `noise_config.json`

Optional per-sample energy dumps live in `energy_per_sample.csv` when explicitly enabled.

## Reproducibility

Determinism defaults to ON. We seed Python/NumPy/PyTorch via `utils.seed.set_seed` and enable deterministic algorithms/cudnn. You can opt out with `--no_deterministic` to favor throughput (non-reproducible). Package/device metadata is recorded in `env.txt`. If dataset downloads fail, the CLI raises a descriptive error so you can pre-download via `datasets` cache.

Note on DDP: automatic GPUâ†’CPU fallback is disabled for DDP jobs to avoid multi-process divergence. In DDP, failures are surfaced for an explicit rerun on CPU if needed.

## é¡¹ç›®æ–‡æ¡£å¯¼èˆª

**å¿«é€Ÿå¼€å§‹**
- [`QUICK_START.md`](QUICK_START.md) - å®éªŒå¿«é€Ÿå¯åŠ¨æŒ‡å—ï¼ˆæ¨èæ–°æ‰‹é˜…è¯»ï¼‰
- [`EXPERIMENT_LAUNCHER_GUIDE.md`](EXPERIMENT_LAUNCHER_GUIDE.md) - å®éªŒå¯åŠ¨å™¨è¯¦ç»†æ–‡æ¡£

**é¡¹ç›®ç®¡ç†**
- [`PROJECT_TRACKER.md`](PROJECT_TRACKER.md) - ç‰ˆæœ¬è¿½è¸ªä¸å®éªŒè®°å½•ï¼ˆå«è·¨ç‰ˆæœ¬å¯¹æ¯”è¡¨ï¼‰
- [`WORK_BOARD.md`](WORK_BOARD.md) - ä»»åŠ¡çœ‹æ¿ä¸é‡Œç¨‹ç¢‘è¿›åº¦
- [`PHASE_RESULTS.md`](PHASE_RESULTS.md) - é˜¶æ®µæ€§ç»“æœæ±‡æ€»ï¼ˆè®ºæ–‡ç´ æï¼‰
- [`DOCUMENT_IMPROVEMENT_ANALYSIS.md`](DOCUMENT_IMPROVEMENT_ANALYSIS.md) - æ–‡æ¡£æ”¹è¿›å»ºè®®

**æŠ€æœ¯æ–‡æ¡£**
- [`docs/experiment_design.md`](docs/experiment_design.md) - å®éªŒè®¾è®¡è§„èŒƒ
- [`docs/code_structure.md`](docs/code_structure.md) - ä»£ç åº“ç»“æ„
- [`docs/FORMAT_STANDARD.md`](docs/FORMAT_STANDARD.md) - æ–‡æ¡£æ ¼å¼æ ‡å‡†
- [`docs/v1_1_0_energy_rework_plan.md`](docs/v1_1_0_energy_rework_plan.md) - v1.1.0 èƒ½é‡é‡æ„æ–¹æ¡ˆ
- [`docs/diagnostics/`](docs/diagnostics/) - é—®é¢˜è¯Šæ–­æ–‡æ¡£

**å®éªŒæŠ¥å‘Š**
- [`docs/reports/`](docs/reports/) - å„ç‰ˆæœ¬å®éªŒæŠ¥å‘Šï¼ˆv1.0.0 ~ v1.0.4ï¼‰

**AI ç¼–ç¨‹æŒ‡å—**
- [`GEMINI.md`](GEMINI.md) / [`AGENTS.md`](AGENTS.md) - AI coding prompts
